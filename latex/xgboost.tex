\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{geometry}

% \usepackage{biblatex}
% \addbibresource{kernel.bib}

\geometry{a4paper, margin=0.5in}
\title{On Time Series Classification}
\author{Manav Seksaria}
\date{\today}
\numberwithin{equation}{section}
\begin{document}
\maketitle

\section{XGBoost}
XGBoost is a gradient boosting framework that uses a number of decision trees to make predictions. It is a very popular machine learning algorithm and is used in a number of applications.

\begin{center}
\includegraphics[width=0.6\linewidth]{images/xg1.eps}
\end{center}

The idea is basically to have a modified decision tree such that the edges are weighted and the nodes have activation functions. Each time an input is passed into the top layer, a decision is made to go left or right. Lets say it goes right, then the weight and bias is factored in on the edge and it reaches a new node where another decision is made. This process is repeated until a leaf node is reached. The leaf node is the prediction.

We can combine multiple such trees and have a forest. The prediction is then the average of all the leaf nodes. This is a very simple explanation of how XGBoost works.

\begin{center}
\includegraphics[width=0.6\linewidth]{images/xg2.eps}
\end{center}

\subsection{Training XGBoost}
The training process is very similar to the prediction process. The only difference is that the weights and biases are updated after each prediction. The loss function is the sum of the squared errors of all the leaf nodes. The weights and biases are updated using gradient descent.

\begin{enumerate}
  \item Data Preprocessing: We will need to preprocess the image data into a format that can be used by the Classifier algorithm. One approach is to flatten the 2D image into a 1D array of length 10,000, where each pixel becomes a feature. We will also need to create a label for each pixel, indicating whether it is faulty or not.
  \item Training: Once we have preprocessed the data, we can train the Forest (multiple trees) classifier on a labeled dataset. The algorithm will create multiple decision trees based on different subsets of the data, and then combine their predictions to make a final prediction for each pixel. The training data should include a set of labeled examples that accurately represent the distribution of faulty and non-faulty pixels in the image.
  \item Prediction: Once the Random Forest model is trained, we can use it to make predictions on new images. To classify each pixel in the image, we can feed the 1D array of pixel values into the model, and it will output a label indicating whether each pixel is faulty or not.
  \item Post-Processing: Finally, we may want to post-process the output of the model to smooth out noisy predictions and ensure consistency across neighboring pixels. One approach is to use a technique called majority voting, where we classify each pixel based on the majority vote of its neighboring pixels.
\end{enumerate}

\subsection{XGBoost in Python}
XGBoost is a Python package that provides a wrapper around the XGBoost C++ library. It is available for Windows, Linux, and Mac OS X, and can be installed using pip or conda.

\begin{minted}{python}
import numpy as np
import xgboost as xgb

# Generate dummy data, this can be given as manual input
X_train = np.random.rand(1000, 100, 100)
y_train = np.random.randint(0, 2, size=(1000, 100, 100))

# Reshape data into a 2D array
X_train = X_train.reshape(-1, 100 * 100) # 100 * 100 = 10,000 here we straighten the image,
   # depending upon the use case some other method could be used
y_train = y_train.reshape(-1)

# Train an XGBoost classifier
dtrain = xgb.DMatrix(X_train, label=y_train)
params = {
    'max_depth': 10, # the maximum depth of each tree
    'objective': 'binary:logistic', # the loss function to be optimized
    'eval_metric': 'error', # the metric to be used for validation data
    'eta': 0.3, # the training step for each iteration
    'nthread': 4, # the number of parallel threads used to run xgboost
    'seed': np.random.randint(0, 1000) # the random number seed
}
num_round = 100 # the number of training iterations
bst = xgb.train(params, dtrain, num_round) # train the model

# Generate new test data
X_test = np.random.rand(1, 100, 100) # this again is random data, but can be given as manual input
y_test = np.random.randint(0, 2, size=(1, 100, 100))

# Reshape test data into a 2D array
X_test = X_test.reshape(-1, 100 * 100)
y_test = y_test.reshape(-1)

# Make predictions on test data
dtest = xgb.DMatrix(X_test)
y_pred = bst.predict(dtest) # this is the [predictions]

# Reshape predictions back into a 2D image
y_pred = y_pred.reshape(100, 100)

# Print the accuracy of the model
acc = np.sum(y_pred.round() == y_test) / (100 * 100)
print("Accuracy: {:.2f}%".format(acc * 100))
\end{minted}

\section{Applied}
Let us state the problem as follows - We have a 100x100 pixel image, and we want to classify each pixel as faulty or not. We have a dataset of 1000 images, each with 100x100 pixels, and we know which pixels are faulty in each image. We can use this dataset to train a classifier that can predict whether each pixel is faulty or not.

\subsection{Data Preprocessing}
We use the following sample python program to represent the receiving of images from the camera and for the inference of the pixels

Steps are
\begin{enumerate}
  \item Receive all pixels with a timeout of 100ns
  \item If the pixel is not received within 100ns, then it is interpolated and considered noisy pixel
  \item All this data is aggregated and sent to the classifier
  \item The classifier predicts the faulty pixels
\end{enumerate}


\begin{minted}{python}
import asyncio
import numpy as np

# Assume the async API to receive sensor data is implemented and returns a promise for each pixel
async def get_pixel_data(pixel_index):
    # Returns a promise for the pixel data
    pass

# Define the sensor data parameters
SENSOR_WIDTH = 100
SENSOR_HEIGHT = 100
PIXEL_DELAY_MAX = 1e-6  # Maximum pixel delay in seconds (100ns)

sensor_data = np.zeros((SENSOR_WIDTH, SENSOR_HEIGHT)) # Initialize an array to store the sensor data

# Define a coroutine function to receive sensor data for a single pixel
async def receive_pixel_data(x, y):
    # Receive the pixel data with a timeout of PIXEL_DELAY_MAX seconds
    try:
        pixel_data = await asyncio.wait_for(get_pixel_data(x * SENSOR_WIDTH + y), timeout=PIXEL_DELAY_MAX)
    except asyncio.TimeoutError:
        # If the data was not received within the timeout, interpolate with averages
        # the multi if else is just to handle the edge cases
        neighbors = []
        if x > 0:
            neighbors.append(sensor_data[x-1, y])
        if x < SENSOR_WIDTH-1:
            neighbors.append(sensor_data[x+1, y])
        if y > 0:
            neighbors.append(sensor_data[x, y-1])
        if y < SENSOR_HEIGHT-1:
            neighbors.append(sensor_data[x, y+1])
        pixel_data = np.mean(neighbors)

    sensor_data[x, y] = pixel_data

# Define a coroutine function to receive sensor data for all pixels in the sensor
async def receive_sensor_data():
    # Create a list of coroutine objects to receive data for each pixel
    coroutines = [receive_pixel_data(x, y) for x in range(SENSOR_WIDTH) for y in range(SENSOR_HEIGHT)]

    # Wait for all coroutines to complete
    await asyncio.gather(*coroutines)

asyncio.run(receive_sensor_data())

print(sensor_data)
\end{minted}

\section{Advantage of XGBoost}
The model can not only classify the pixels as faulty or not, but also tell us why it classified the pixel as faulty. For example the model may say that the pixel is faulty because the pixel signal value is below a certain threshold

Every once in a while we just have to pass in small datasets to update the distribution of the model, this is called incremental learning, there do exist incremental SVMs also but they are not as good as XGBoost for this specific problem. So say if we trained on 10,000 images we can just pass another 50-70 images (say) to retrain the model and update its distribution.

XGBoost is itself very resilient to noise and overfitting and has been shown to tolerate upto 1-1.25\% noise in the data with no significant drop at all in accuracy. So even if we have a noisy dataset, we can still use XGBoost to train a model and also do inference on slightly noisy data. Additionally this also means that small shifts in distribution of the data will not affect the model much since they can be passed of as noise

% \printbibliography

\end{document}
