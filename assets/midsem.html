<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="https://api.nukes.in/css/global.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/reveal.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/theme/simple.css" id="theme">
</head>

<body class="reveal">
  <main class="slides">
    <!-- Slide 0: Objective -->
    <section>
      <h1 class="ƒ ∆-ct" style="align-self: center;">
        <img src="https://upload.wikimedia.org/wikipedia/commons/5/51/Qiskit-Logo.svg" height="150">
        bjective
      </h1>
      <div>New Kernels for QSVMs</div>
    </section>

    <!-- Slide 1: Introduction -->
    <section>
      We define regression as finding a $w$ such that we can predict new values of $y_i$ given $x_i$ as
      <div class="ƒ w-100 ∆-ar">
        <p class="dull" style="font-size:1.25rem">
          <i>Linear:</i> $ Y = w^T X $ &larr; (easy)
          <img src="https://upload.wikimedia.org/wikipedia/commons/b/be/Normdist_regression.png" height="250"
            class="p-rel" style="top:20px" />
        </p>
        <p class="dull" style="font-size:1.25rem">
          <i>Non-Linear:</i> $ Y = w^T \phi(X) $ &larr; (hard)
          <img
            src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Loess_curve.svg/1920px-Loess_curve.svg.png"
            height="290">
        </p>
      </div>
    </section>

    <!-- Slide 2: Linear Regression -->
    <section>
      How do we do it? Lets look at the regularized value of $w$
      (from $L(w) = \frac{1}{2} \sum_i (y_i - w^T x_i)^2 $)

      <div class="ƒ ∆-ar">
        <p>
          <i>Linear:</i> $$ w = (X^T X)^{-1} X^T Y $$
        </p>
        <p>
          <i>Non-Linear:</i> $$ w = (\phi^T \phi + \lambda I)^{-1} \phi^T Y $$
        </p>
      </div>
    </section>

    <!-- Slide 3: Linear Regression Problem -->
    <section>
      In $ w = (\phi^T \phi + \lambda I)^{-1} \phi^T Y $, the matrix $\phi^T \phi$ is painful to
      calculate so we can use some mathematical jugglery refactor it from
      <b>$$ w^* = (\phi^T \phi + \lambda I)^{-1} \phi^T Y $$</b>
      to
      <b>$$ w^* = \phi^T (\phi \phi^T + \lambda' I)^{-1} Y $$</b>

      it turns out that $\phi \phi^T$ is much easier to calculate
    </section>

    <!-- Slide 4: Proof of Kernel Transf -->
    <section style="font-size: 16px !important;">
      <b>Optional Proof:</b> <br />
      Let us now define the new loss function along with a regularization term
      $$ J(w) = \min_w \sum_{i=1}^n (y_i - w^T \phi(x_i))^2 + \frac{\lambda}{2}
      ||w||^2 \implies
      w^* = \frac{1}{\lambda} \sum_{i=1}^n (y_i - w^T \phi(x_i))
      \phi(x_i) $$ For sake of simplicity let us define a variable $\alpha$ such
      that <br />
      .$ \alpha = \frac{1}{\lambda} \sum_{i=1}^n (y_i - w^T
      \phi(x_i)) $. We can now write the weight matrix as <br />
      $$ w^* = \sum_{i=1}^n \alpha_i \phi(x_i) = \phi^T \alpha $$ Let us now
      substitute this into the loss function and get <br />
      $$ J(\alpha) = (y - \phi \alpha)^T (y - \phi \alpha) + \frac{\lambda}{2}
      w^T w = y^T y - y^T \phi \phi^T \alpha - \alpha^T \phi^T y + \alpha^T
      \phi^T \phi \alpha + \frac{\lambda}{2} w^T w $$ We can see that $\phi
      \phi^T$ is a repeated term. Let us define this new matrix as $K$ such that $$
      K = \phi \phi^T = \begin{bmatrix}
      \phi(x_1)^T \phi(x_1) & \phi(x_1)^T \phi(x_2) & \cdots & \phi(x_1)^T \phi(x_n)
      \\ \phi(x_2)^T \phi(x_1) & \phi(x_2)^T \phi(x_2) & \cdots & \phi(x_2)^T \phi(x_n)
      \\ \vdots & \vdots & \ddots & \vdots \\ \phi(x_n)^T \phi(x_1) & \phi(x_n)^T \phi(x_2)
      & \cdots & \phi(x_n)^T \phi(x_n) \end{bmatrix} $$ This matrix has two very
      important properties. First, it is symmetric and second, it is positive semi-definite.
      (This also means it is invertible which $\phi^T \phi$ MAY NOT). We can substitute
      all $\phi \phi^T$ with $K$ and also $K$ with $K^T$ and get <br />
      $$ J(\alpha) = y^T y - 2 y^T K \alpha + \alpha^T K^2 \alpha + \frac{\lambda}{2}
      \alpha^T K \alpha \implies \alpha* = (K + \frac{\lambda}{2} I)^{-1} y = (K
      + \lambda' I)^{-1} y $$
    </section>

    <!-- Slide 5: Kernel Trick -->
    <section>
      <p>
        What this effectively lets us do is,
      </p>
      <p>
        We need to do a Non Linear transform so that we can then draw a linear line through the transformed data, and
        then
        un transform that
      </p>
      <p>
        This gives us the end result directly
      </p>
      <div class="ƒ ∆-ar">
        <img src="https://miro.medium.com/v2/1*zWzeMGyCc7KvGD9X8lwlnQ.png" height="200px">
        <img
          src="https://www.researchgate.net/profile/Marouane-Hachimi/publication/340610860/figure/fig4/AS:880021191286786@1586824810950/Non-linear-classifier-using-Kernel-trick-16.ppm"
          height="200px">
      </div>
    </section>

    <!-- Slide 6: Kernel Examples -->
    <section>
      <h2>Some examples of kernelised transforms:</h2>
      <div class="ƒ ∆-bw ƒ∑">
        <p class="w-50">
          <b>Linear Kernel:</b> $ \phi(x) = x $ <br>
          where $k(x, x') = x^T x'$
        </p>
        <p class="w-50">
          <b>Polynomial Kernel:</b>
          $k(x, x') = (x^T x' + c)^d$
        </p>
        <p class="w-50">
          <b>RBF Kernel:</b> $ \phi(x) = \sum_{i=0}^{\infty} c_i x^i $ <br>
          where $k(x, x') = e^{-\gamma ||x - x'||^2}$
        </p>
        <p class="w-50">
          <b>Arbitary Example:</b> $ \phi(x) = [x_1^2, \sqrt{2} x_1 x_2, x_2^2] $ <br>
          where $k(x, x') = (x_1 x_1' + x_2 x_2')^2$
        </p>
      </div>
    </section>

    <!-- Slide 7: QSVM Intro -->
    <section>
      <h1 class="†c">$|QML\rangle$</h1>
      <div>Heading to QSVMs</div>
    </section>
  </main>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/reveal.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/plugin/math/math.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/plugin/markdown/markdown.js"></script>
  <script>
    document.querySelectorAll( 'section[md]' ).forEach( e => {
      // converts 'md' to revealJS compatible format
      e.setAttribute( 'data-markdown', "" );
      e.removeAttribute( 'md' );
      e.innerHTML = `<script type="text/template">${ e.innerText }</` + `script>`;
    } );
    //
    Reveal.initialize( {
      history: true,
      transition: 'linear',
      showSlideNumber: 'all',
      fragments: false,
      hideCursorTime: 1e3,
      viewDistance: 3,
      plugins: [ RevealMath, RevealMarkdown ]
    } );

    // DEV MODE:
    setTimeout( () => window.location.reload(), 12e4 );
  </script>
  <style type="text/css">
    ul {
      margin: 0 !important;
      padding: 0 !important;
    }

    li {
      list-style-type: none;
      font-size: 1rem;
    }

    img {
      object-fit: contain;
    }

    .reveal .reveal_section {
      position: relative;
    }

    .reveal .caption {
      position: absolute;
      bottom: 0px;
      left: 50px;
    }

    .dull {
      opacity: 0.75;
    }

    .MathJax {
      pointer-events: none;
    }
  </style>
</body>

</html>