# QML - Maria Schuld
So, okay, cool,Welcome everyone,Yay to the first Quantum Machine Learning Meetup that we started on a whim,And today's speaker, of course, we all know who she is, but I'll leave that introduction to later,So I just want to tell you quickly in two minutes, the reason we're doing this, so some background information myself and a few other people were mentoring as part of this Quantum Open Source Foundation initiative, where we try to educate and help other people to get, you know, started with quantum computing,And a lot of people were very interested in quantum machine learning, but we couldn't get enough mentors to help them,So we decided to create an event and hence this meetup to facilitate this and foster this interest in quantum machine learning and kind of build a community here together

So this is really an informal meeting where we have a researcher discussing research and a discussion afterwards,And I just want to just mention that this is in partnership with a few people, right? So it's not always one behind the scenes,So Antal unfortunately couldn't be here, but he's very much also part of this initiative and Arusa as well,And Arusa, Antal and I, we all met, we met before in person, but we also got together again virtually in this Quantum Open Source Foundation mentorship initiative,So this is how this whole idea came about,And a big thanks to Francesco of course, who you'll see on the panel there, because he very kindly also helped to sponsor this event and allowed us to use their Zoom license, which is why we can have so many people attending today,So that was really nice

Okay,And then I will hand over to Francesco to introduce our wonderful speaker,Yeah,Thank you very much, Amira,And good evening, everyone,So it is my real pleasure to introduce Maria,And Maria gave me the freedom to say whatever I want about her

So she's in a very dangerous position in this moment,No, but it's really a pleasure to introduce Maria, because all we learned in my group about quantum machine learning was only thanks to Maria,Because when Maria started her masters, nobody knew anything about quantum machine learning,And she was brave enough to jump into it and start to develop the field,And the rest is just history,All of you probably already have read something published by Maria,And all of you have probably read the famous book by Maria

And you're probably all waiting for the second edition of the famous book,And now, Maria, thank you very much for offering to be the first speaker in this quantum machine learning meetup,And we are all very keen to hear and see how you're taking stock of machine learning in a very critical perspective,So thank you very much, Maria, for being with us,Well, thanks so much,Okay,What you didn't say is that we

,,Sorry, let me let you share the person enabling me to study this and supporting me with a lot of funding and psychological support and research support,So obviously, the thanks is from my side,Okay, let me start sharing my screen,And let me start,

,One more thing to add, sorry, before we start,Also, just so that people know, please pop your questions in the Q&A as you go out or in the chat and we'll get to them at the end of the session,Cool,Thanks,Now, I have a couple of disclaimers before I start,The first one is the title of the talk already shows kind of the mindset that this talk is in

And those of you who know me know that I'm kind of constantly oscillating between euphoria and skepticism,Sometimes I can even like in one hour be like super happy about something and not really so much,And at the moment I have my half empty cup here,So this is like a critical perspective,The second disclaimer is it is not a critical perspective from an investor point of view,So if there's a question of should we invest into quantum machine learning or will this be the next big thing for quantum computing? This is not the questions I want to analyze here,I want to actually do this from a very much research perspective

So the question is, what are we doing? At least in my humble opinion at the moment, very much wrong in research and which question should we be very bold about and courageous about, especially when we are young students just starting with a PhD,And then the third thing, the third disclaimer is a bit problematic,So I thought we would have like 20 experts here now,A few researchers already in the field have written papers and we would kind of discuss things from this perspective,I didn't know so many people come in,I assume by the numbers that most of you are actually not researchers in the field,So let me make this very clear

This is a technical talk, but since I want to draw an argument and arguments are just understandable by human logic, I hope that a lot of the general argument that everyone can understand, but maybe the details of like really going and understanding where I'm coming from, maybe this is something that only then a smaller part of the audience can do that doesn't stop you from asking questions,So even if your question is coming from a very, very beginner's level, we're super happy to share them amongst ourselves,Cool,What I thought, I thought like how do I do this? And I thought, let me do as follows,This is actually an abstract that I like very much and I saw on the archive yesterday,And I want to read it to you for now,Quantum neural networks can exponentially speed up machine learning

So quantum neural networks are variational quantum circuits that similar to classical neural networks can be trained and used for prediction tasks in machine learning,In this paper, we produce a new maximally expressive variational answers, suggested training strategy to avoid their implant post-hearing training,And we show that the answers gives rise to quantum models that are classically intractable leading to an exponential separation between classical and quantum,Funnily to show with simulations that the quantum model learns to classify MS data well and implement the circuit on the IBM quantum experience with two qubits,Okay,As you might have guessed already, this is a completely fake abstract,But I think if I was a generative model, like kind of like making up a quantum machine learning abstract, this will be a really good shot at it

And maybe the good thing or the sad thing almost is I think any student who actually writes a paper like that, they probably send it to nature at the moment,I mean, you might not get through the editors might pop the coin if you even make it through the first round or not or whatever,But it contains all the buzzwords in the field,It contains a lot of little logic that we follow in the field at the moment,And it definitely contains what people consider as the goal at the moment, these exponential speed ups of machine learning in general,And so what I want to do in this talk, I will go through every of those sentences,So four sentences, basically

And I want to just share with you my concerns about them,And yes, so this is very critical,I hope I don't start ranting like an old lady,But so if you are researching in this way, I'm not saying that this research or this work is bad,I think this is like extremely hard research to write, for example, a paper like that, I will be like super proud if I could do this and prove all those things,I'm just starting to think if in the long run, these questions, we should tackle them a bit differently,So that everything

Oh, by the way, I had so much fun making up those names here,So they are also completely made up,And I don't know if there's a Harvard Center for quantum computing,And I thought, what is the what is what are the names that make you trust this paper most? And I thought, like, it must be three authors, they must be all male,And it's a Chinese name, a Russian name and American English name,And I even have like kind of characters for them,But I might tell you after if there are no questions

Okay, so I have two slides for those who have no idea what I'm talking about in general,Let me just go back and the two slides explain basically the problem of machine learning, which I need a little bit to build my arguments and what we think or what we use as a quantum neural network,And just to explain more quantum neural network is just one model or one approach to quantum machine learning that we're investigating at the moment where you take a machine learning model like a neural network, you take it out of your pipeline, and then you plug in a quantum computation,And this quantum computation depends on physical parameters that you can train,So that's everything you actually need to know,Going one step further,So let's talk about machine learning very briefly

So what's the problem we're trying to solve in machine learning? Machine learning is a lot, but in most cases, you have some data,So this is kind of a data source, you can think of it almost as a probability distribution of a data, or like a finite set of data samples, for example, 100 images,And then you have a model class,And this model class could be a function, or it could be a probability distribution,So the function would be often used for supervised learning where you take your data inputs, and you predict some label or some class or some value,And this will be a supervised learning task,And then if you have a probability distribution, you can also think of the data being sampled from that distribution

So model can be like very different things,But very often we're thinking of models as some kind of function class that's parameterized by parameters,And the third ingredient is then a cost that tells you how well is any of these models,So if I choose my parameters, doing on a finite set of data samples,And now what's the task in machine learning is actually, this is really interesting, right? It's you find the model that minimizes or a model that minimizes the cost that I've chosen, which will be something like that has to do with a task model,On all data I could see,So this is something we can never access, obviously

And so machine learning is basically this weird thing of solving a proxy problem,So the proxy problems find the model that minimizes the cost on a training set,And obviously, if you do machine learning, you optimize some neural network on the training set,And then what you actually really want to know is how well is this model doing on all the data I haven't trained it with,And machine learning is this beautiful, and we have to understand this when we do quantum machine learning, is this beautiful mathematical problem of having something and being able to act on it with algorithms,But what we actually want to do is something else,And a lot of machine learning theory asks, what is the relation between this problem and this problem? And then the second building block is quantum neural networks are these variational circuits

So they're quantum circuits,So think of them as little physical computations by quantum mechanics or quantum information processing that we use as machine learning models,And we often have this picture language, which is actually not easy to understand, I think, when you start,But you can read them almost like diagrams in classical machine learning or in classical computation, where these are like units, like there will be your bits, but we have quantum bits,And then you apply some operations on these bits,And on the right, it's time, so to speak,So you apply the green block and the purple block

And then you do some measurements,And here I will exclusively talk about quantum computations, where you kind of average over your measurements,And what you get out of quantum computation is what we call the expectation of an observable,So basically, the average of your measurements,So this is a deterministic value for now,And we're not talking so much about unsupervised generative models,We're talking about supervised classification, so functions that we try to learn

OK, let's start with the first slide,I think Amira will like this,So I'm kind of often discussing with people that I find the name of calling this quantum circuit that we train instead of machine learning, that we call this quantum neural networks,And actually, I think I changed my mind a little bit about it,So I think it's really good to call them quantum neural networks, because there is no better name,Like, otherwise, I think you can call them supervised, parameterized variational circuits,Or I think there are lots of lots of different terms in papers

But more and more, this one is used,However, why I start basically as a warmup with this kind of issue I take is that many people take this very, very serious and think we literally just replace something classically with something that's more powerful, which is the quantum part,And they were super careful,And I give you a couple of arguments now,The first one, one,So if we really compare what mathematically is done in these structures, neural networks, as most of you know, are composable, efficiently differentiable functions that transform inputs by chain of linear and nonlinear operations,So the inputs are usually tensors or vectors or something like that

And so these things basically are kind of this modular structure of neural networks,And this is like the beautiful optical representation you get,Variational quantum circuits give rise to composable,So this is still the same differentiable functions that map inputs to quantum states and perform linear operations on these states,So I know this is like quite a mouthful, but something completely different is going on here,So you don't have this linear, nonlinear operation,You have one big nonlinear operation that maps your data into high dimensional spaces, and then it performs linear algebra

So I try to write down the mathematical description of a circuit like this,So the circuit plus running it a lot of times and averaging in terms of like how quantum theory like tells us how to compute these expectation values in mass,So most of you will notice,So basically you start with an uninitialized state of qubits, and then you apply like some circuit that encodes data,So after this, you have a quantum state, which we can represent by this high dimensional vector that somehow depends on X,And then you have kind of this quadratic form here,So why do I say, so this is more like a bit of a detail, but why do I say map inputs to quantum state and perform linear operations? This is clearly not linear

There's something quadratic going on here,You can actually rewrite this by you take a trace of this, which is the same result,And then you kind of take the cyclic property of the trace and shift this to the end,And you see that so there's a bit of like people math,But if you write the last part is the density matrix, actually you have only linear operations,So there's a lot of details why I can say this, but the statement is accurate,So now you can say, okay, there's not really the maths of neural networks going on

And it's really hard to actually get this nonlinear linear chain working,And lots of people have tried this and it's not entirely clear why one would do this,But there's a second big problem,And this is the word that's missing here,And this very nearly like dawned on me like very recently, other people have understood this for a long time, that this is a problem,So let's go back efficiently differentiable,And here we only have differentiable

So having these circuits being differentiable, which just means we can train them with gradient descent methods that are used in deep learning, for example, this was already a core achievement,So why am I not happy with this? And the point is the following,So assume that the number of parameters of a model grows linearly with the number of training samples,This sounds a bit technical, but that just means if you want to learn from more data and bigger data sets, you probably have to increase your model size as well,So in neural networks, this is definitely true,But if you think that this is true, then you have these two scaling behaviors between neural networks and variation circuits, your networks scale roughly linear with M and your variation circuits, they quadratically with M,The first M is basically your training algorithm just scanning for your data, which is the same as in your networks

The second one comes from that our differentiation rules that we use scale linearly with a number of parameters,So the parameters grow, the bigger your problem grows, you have the quadratic scaling with M,And what you get is a really, really nasty scaling behavior and something that in classical machine learning, no one would ever accept as a good model,So the reason, so I would go so far as to say, the reason why neural networks are used is that this red line is so nice,So if we cannot match this red line by the physical principles of training our models, then, or we can show that this first statement is not true, then we have a problem,And I have a feeling no one is looking at this, although this is like the most obvious bottleneck of these whole variation methods,And then there's obviously like, so this idea of that mathematically speaking, we know by now that these quantum neural networks are actually kernel methods, like for those of you who do machine learning, like support vector machines

So as I said, we map data to high dimensional spaces, and then we've defined linear decision boundaries,So mathematically, this is actually like a bulletproof comparison,So I feel that this link, which has actually given rise to, so I just mentioned these two papers here, these are always the archive numbers, so you can get them for free those papers,And the second one here that I will get back to now, where, in my view, at least like some of the most advanced studies in understanding how quantum machine learning works in a separated from classical machine learning, and both of those studies use kernel methods,So they don't use notions of neural networks, because it's just too hard to analyze,So I feel by calling them quantum neural networks, we take away from young students to actually understand that the theory that can help them understand these methods lies somewhere else,So that's the point

Secondly, I'm really not ranting here, but what are the good answers? And this is my second big takeaway, if the first one wasn't big enough, because the first one is just basically a changing perspective,Maria, so we have a very nice question, which I think just ties into what you just said,Someone asked, what is the intuition or reason behind the quadratic versus the linear scaling with parameters in variational training versus neural network training? So as I said, so this quadratic scaling is basically just so if you want to like say more precisely, it scales linear with m, and then again, linear with the number of parameters, I'm just like to simplify that just put here like the number of parameters probably grows also with the number of data,So it's this linear scaling with a number of parameters,And the intuition behind this is that our gradient or differentiation rules that call, we call them parameter shift rules, they basically take each parameter in your quantum variation circuit,And for each parameter, they have to run two circuits to estimate the partial derivative with respect to this parameter,So you've got a billion parameters, you have to run 2 billion circuits

And now you think, okay, this is logical, right? But the point is that neural networks don't do that,So TensorFlow and PyTorch are based on automatic differentiation, which is based on recycling values of gradients so that not for every parameter, they have to like kind of run the whole network again, or backwards or forwards, but often you can just like in one forward run and one backward pass, this is this backwards and forwards that you sometimes see in the programming languages, you can actually estimate a whole gradient of a billion entries,So that's the, that's it,So basically, neural networks really are trainable efficiently,And the reason for this, this computational details, actually not so much mathematical details,And they're also not included,Actually, I'm happy if, you know, if you ask your questions now, because we've got some time together

And if you don't mind that we don't have a long Q&A at the end, I think it's nice to also get challenged on those points, because obviously, I'm also, you know, speaking against a lot of, so if these critical points are true, then 95% of the papers we see on quantum machine learning are doing the wrong thing,So this is like a, I want you to understand that this is like a statement that can be controversial,And I'm happy to hear if you don't think so,Okay,So I'm going to interrupt the end,Then the second one, and this is as important as the scaling problem,And, and again, I admit that I only understand that this is really a problem at the moment

So challenge you to go to the quantum machine learning literature where they take these like circuits,And as I showed before, maybe I should just say what I mean by answers here,As I showed before, they often have a block that encodes data, and then they have got some kind of quantum algorithm that is trainable,Sometimes they also mix, but that's not so important,When I talk about answers, talk about what is actually this W and what is this S in terms of quantum gates and arrangements,But for now, like one can focus on this block here, because this is really the trainable answer,This is the data encoding answer

But I challenge you to go to quantum machine learning papers for variation circuits and actually go to the point where the people say this is the answer we use,And I include my own papers in this as well,And then check why they choose the answers,And we have absolutely no, I don't know, argument for those things,We just always like saying the paper we chose and how we efficient answers with three and ten dollars and five rotations,And then sometimes it's expressive,So we have to analyze what is happening with parent plateaus

But in classical machine learning, no one goes there and just says, let's take a random function class and to see how it does,So the idea of neural networks will obviously like deduced from how things work in the brain,The idea of support vector machines was deduced from statistical learning theory,So it was very, very specific what people did then,We just like happened the dark in my mind,And this has to stop if we want to do something interesting,And so actually, I would say that so far we're guessing our answers, even the general structure, you know, we just use what we know, we know QA, we know like heads and networks, so we just use those

And so if you go to these papers and you check, then you find that there is actually one argument that people give,And this is the expressivity,So if anyone says something about which is this answer, because they often talk about we want a very expressive answer,And what they mean by expressive is that the so basically by repeating this answer, it's a lot of times you can reach quantum states and further and further corners in your Hilbert space, you can basically cover like really, really different computations in this answer,And there's like one point that I think I made a lot of times and also colleagues of mine made a lot of times, but students often don't understand until like a couple of years into their research,And I would probably also not really see this, I would start now,But it's really important

The circuit as expressivity is not the model expressivity,So there are two notions of expressivity here that we must conflate,And I want to demonstrate this with a very quick example,Imagine the quantum machine learning model or quantum neural network,Yeah, I said it before for the first time in my areas,And now I say like, I tell you what I do in this data encoding part and the variational part,So the data encoding part is just the palli rotation, single data point in future

And then I have these three angles,So just as a optical representation, what the model does, it kind of maps the data to a block sphere, if you want so, and then each data point by itself, obviously, but I just represent them all together,And then it rotates this data set somehow, and then kind of does a measurement,And, you know, this is your supervised learning model,And now the point is, so basically, this second part for a single qubit, the this one here is a general rotation, right around three angles,So this is the most expressive circuit I could have on a single qubit in terms of like parametrized circuits,But the model it gives rise to, so if I just compute what is the expectation of M, and then I call this f of x, and this teacher here are those three parameters

So what is actually the model that the circuit represents, you get a super simple answer here, you get this is just cosine of x minus and you see here, for example, so first of all, there's only cosine sine functions, you can even simplify this to only have a cosine function, yeah, so it's literally a single cosine function with a single frequency, you don't even tune the frequency here,And the third parameter actually even vanishes,So this is what I mean by this is a very stupid model, but a very expressive circuit,Now you could say, okay, it's a single qubit, maybe this is why the model is so stupid,But it turns out you can take as many qubits as you want, as long as you embed in the same way,If you have the craziest deepest quantum computation here that you want, you always get something of this form here,And that's proven in all that's first, I think, was noticed in this paper here

And then check in this paper, there was another paper like before ours that kind of used a very similar formulas and to show that this is actually just something with to do with query series,Then lastly, and then kind of a wrap up this point, you could now say, okay, the answer, okay, it's not the circuit expressivity of the answers that make the model expressive,Somehow it's still the circuit expressivity of the embedding part,And to some extent, this is true,So if you tune this embedding, you can make this function more and more expressive,But however, there's now the next point,So not only is it really hard to talk about these expressivity things, we don't even know if expressivity is really important

And this is a point that I discuss a lot with Amira, for example, and also there, please challenge me,And I want to like, maybe have a bit of a strange example, maybe it takes a leap to really understand this,But let's say in this, let's, let's kick out the whole rest, the measurement in the circuit, we just look at the embedding, how is data represented on a quantum computer? Let's take something called basis embedding,So I've got data of classes green and pink, and I map the data to basis states, computational basis states,So now, this is a very expressive embedding, you could say in a quantum circuit sentence, right, because I map data to really far corners of my Hilbert space,But however, you have, unless you have some other clever way to extract information, that only the inner products of these states will give you absolutely zero information, what class they are in,So for example, if you consider these as data points, and you would take like just the inner product, which I wrote in the really like funny way

So let's say the overlap of these states, obviously, these are basis states,So you will see that they only have a one overlap overlap with themselves and zero with everything else,So you have an embedding that unless you use another algorithm that does something interesting, you have no idea how to extract them,But maybe in a more general machine learning manner of speaking, you could say that model expressivity is not necessarily a good thing is actually something that is known since decades and decades in classical machine learning, where we know that a good model is a trade off between the simplicity of the model class, and how good I can get my training error,So basically, if my training error gets very, very small, it means my model class gets very, very expressive,And so I have to either deal with a very bad training error, or with a very, very quick, let me see, expressive model class, but good models kind of find this, the sweet spot between the two,So there is a sweet spot of expressivity

So this is not a measure you should ever maximize, or we need to understand how to maximize it,But it's not necessary that a good model is the one with expressivity in whatever measure is maximized,Are there any questions about that? Oh, sorry, what does ABC stand for? Yeah, okay, so you guys in the chat, if you, if you can answer each other's questions, that's also cool,And anything that's still standing out,There was a nice question about embeddings, if I can mention it quickly,So the question goes, in the IQP embedding, the encoding layers are next to each other, which seems to mean that the effect, the effect of adding more layers is simply increasing the frequency of the cosine kernel,So if this is the case, then I lost this question

I think is this is this, if this is the case, I think the question was saying that then just repeating these layers of encoding is better,Right,So, yeah, so now this is exactly the point,So you are right that there's a mechanism,So not always, but in many, many cases, making an embedding and then repeating it makes the model class here richer,Why I'd say not always, you have to be really careful because sometimes, so basically what happens here in this, in this simple model is that this purple circuit defines what the ABC constants are,And the green one defines what these, these basically basis functions are, or these trigonometric functions

And you can build free series out of these models and some things,And so you have to make sure, yeah, sorry,So just so I understand, repeating these encoding structures make these functions more rich, right? Yes, makes the function class richer,But so everything in machine learning is about general, sorry, regularization, which means you try to simplify the model class so that you don't overfit, right? I mean, regularization terms and all of those things,And in terms of your model class and in kernel methods, this is really beautifully represented with this reproducing kernel Hilbert space where you can really describe how these models, the model class looks like,Anyways, but this model class, if it's too expressive, it starts overfitting,And there's a very technical and very strange and not entirely understood reason why huge neural networks don't overfit, but it's very complex

It's not just because they're big, they're good,They're big and then training happens and then somehow really good models are picked,So something very strange is going on there,So all I'm saying is just repeating the embedding, yeah, makes your model more expressive, but maybe at some stage you will have very high frequencies in your model and they start overfitting your data because now you can wiggle around in your model function space too much,So this question is actually really important,Where's the sweet spot of expressivity in the embedding and in the variation part in the quantum model and they both play together,So this is like, yeah, really good

Thank you,Now I've got the third or four points, which is like third and this is maybe the hardest for me because I'm not, I think I've never been really, really good,Maybe this is why I'm not so interested in all the other way around, but I'm never really good in doing these proofs to show quantum advantages,So this kind of like going through tons of calculations to get like some lower bounds on something and then prove that this one algorithm is faster than the other kind of always lose interest,So this is why I'm not really equipped to speak a lot about the super technical parts of like quantum advantages and speed up,However, there's one thing I start understanding now that the question of quantum speed up in every field almost has to be defined very precisely,And I'll give you a couple of examples of what I mean by this

So the first one is that in the early quantum machine learning papers, people have often said, I have a model and now my model is a quantum circuit,So it can compute things like classical models can't compute,Hence my quantum model is classically intractable,Hence my learning algorithm, or I have somehow sped up learning,And I think there's a disconnect between having an intractable model and speeding up learning,And the first iteration of understanding this is actually the problem that we had here,So what we actually speeding up in this kind of argument that we have is the model here, right? And then you can say, okay, so if you speak, if you have a model that's classically intractable, in theory, you could just now construct a data set or model class that you want to learn and say, this is the ground truth I want to learn

So you constructed a case where your ground truth is classically intractable,This is often how the argument goes,However, as I tried to explain, like there's more to the learning problems than just having fast models, because you have to, you have to prove a lot more things to show that you really sped up learning,The first thing you have to show is, for example, isn't there another classical model that could also learn really nicely, like this data set? The second point is, can my quantum computer actually learn this model? So my quantum computer can represent this model, but no one tells me that it can learn it like in from like a small data set,And the other point is like, if my quantum computer learns this model, like this is generalized nicely to other data,So there's this whole question of what are we actually speeding up? And this changes really much what's possible and how our statements are,And if you are from industry, obviously, this is really important because if I say I speed up learning, I think to speed up learning, that's almost impossible because the problem is too big

We know there's no free lunch theorems and stuff,This is a really large problem to solve,So you can only solve very small parts of it,And then can you really call it a quantum advantage for learning? So I think this question is that quantum advantage for learning is completely ill posed,And there is absolutely no good answer to this, if we continue posing it in this way,Just a couple of like going a little bit deeper into some of these points,So for those of you who might not be so much like in computational complexity theory

So what we mean by exponential speed ups,So they are first of all asymptotic statements,So look at this left picture here,So it means that the runtime of your algorithm f grows with x, which is something you're interested in, for example, the precision you want to get are the size of the data, like hyper parameters of your problem,So to speak,And you say kind of that it is an O of G of X if there is some scalar value lambda so that all values,So there is basically one X from which onwards

So this is the asymptotic part of the statement,You will honor,Hang on,We don't need it in this formulation,Sorry,So there is a lambda in which basically f of X is always smaller than lambda times G of X,And this is this thing that people always talk about constant in your algorithm

So obviously, if your classical machine learning model, how can I describe this now? So let's say your quantum algorithm, your classical model has a runtime like this here and your quantum algorithm has a runtime that's up here,It might have a better scaling, but it has such bad constants that this is really, really not practical,So this is what people always say like, OK, this is an asymptotic statement,So this is really important to me,What I wanted to talk about is more that they are relative statements of speeding up something,You need to know what's the something,And I thought a really nice paper is actually this one here

I won't see it very much excited or something, but I mean, it has a lot of citations, but there was a whole bunch of people who came together and and talked about what is actually a speed up, a quantum speed up,And they had these different classes,So the particular of these classes are not so important,But just to give an understanding that you have to ask which class your statement in,So is it a provable quantum speed up? So there can be no classical algorithm that performs better,That would be short, for example, Grover's algorithm,Or is it a strong quantum speed up? It compares quantum algorithms with the best known classical algorithms

This will be short for them in this category,But if we say something like we speed up your networks, we actually kind of like compare to specific algorithms or you can say conceptually equivalent algorithms,But we don't know if there's another classical algorithm that is maybe solving the learning problem as good,So this relative part is really important in these statements,And then my last little point about this is that not only what is this relative statement, what are you speeding up? What are you really talking about there is important, but actually this advantage question has so many more layers to it,And depending on how you choose them, you make your life very complex or very easy,And you can say, oh, I've got a quantum machine learning speed up, but you just chose all the easy settings

I'll give you just a few examples here,So this one we spoke about already,So do you solve the learning problem or do you speed up a specific algorithm? So is the best for the general task or just very specific? Then obviously the question of is your algorithm for near term or for full tolerant? It's much easier to come up with full tolerant algorithms because you don't have to think about noise and short circuit depths and stuff,So this is really hard here, but obviously also nicer because these algorithms can run quicker or we can implement them faster in terms of the years we have to wait until we can use this,Then there's also this idea of for the size of data we're looking for, is this an open topic statement or not? And then also there are very different arguments you can give,You can say, okay, I've got a proof, or you can say I've got a benchmark that shows very often this is really better,In my benchmark, quantum neural is better than a classical neural because it's something completely different obviously

Or you have a bound that you don't know how tight it is,And then another one that's super important is, is it actually a realized data set or a pathological problem? And I wanted to give you basically two examples of papers that I thought were very, very important in the last couple of years that talk about quantum speed ups and to just show you that they take these questions from a completely different perspective,And then I'm kind of like done with this section,And if you have questions, I'll also take those,And the last section is very short,So the first one was actually, I would say it was the paper that kicked off quantum machine learning,So it was the first, I think it was in PRL and it was kind of the first time someone really took this on the topic and said, okay, how can quantum computers really like make a difference here? And it was Patrick Ravensthorst with like Mathsud and Seth, and they are really experts and they're absolutely fantastic in this whole speed up ideas that I'm really good at

So you can really trust them that they did their homework from the quantum side very well,And so what this paper basically does, it shows that you can take in NHL and linear algebra subroutines for quantum computer that if you have data basically given as a quantum state, so if you have a very efficient way to feed things in, and then you've got a full-tolerant quantum computer, you could in principle do an inversion, a matrix inversion, instead of in quadratic time and linear time,And what is this? No, no, sorry,If you can encode your data like super efficiently into amplitudes of a quantum state, you can get logarithmic runtime instead of a linear runtime in the size of the data,So it's something very, very constructed and very, very with lots of terms and conditions that before,But if you might use this and go back here, so they speed up basically a specific algorithm, they use SVM and make it faster,This is definitely a full-tolerant algorithm

So it's the class that we can't do for a long time,We have asymptotic arguments and they have a proof, which is really nice,But so one problem is to load the data to fulfill the requirements of the proof,They need the data set to fulfill a lot of very, very specific conditions,And then the problem becomes very pathological,So in the sense that, for example, you need your data to be sparse or to be low rank,And the problem is also then maybe the speed up disappears very soon if you actually assume these assumptions too much

So there's this kind of trade-off of your assumptions that are very strict,And if you can actually show that there's no classical algorithm that you can do,And then the second paper, this is more recent,And I think this really showed how the whole field has evolved,Because this one here now took all the experience from the last years to ask the question on a much deeper level,And I won't go into the details, but it's basically the idea that we use Scholz algorithm or some subroutines from that to map data into quantum states,And now we can prove that these quantum states are separating data classes that we very carefully construct, very pathological problem, and separate them very nicely

So what they show is not only that there's a model that classical computers can't simulate, but they can also show that the quantum computer can learn this model and that it will generalize well,So they really tick all the boxes,So this is a very, very beautiful paper in this regard,And to go through those points again, so this is not a specific algorithm,This is a general learning problem they found that quantum computers can do well,So they really move this to the hard side here,Still full tolerance

So I don't think a lot will be provable in linear term time, at least not the level we are now at the moment,Still asymptotic scaling, it's a nice proof,Again, it's completely pathological problem, much more even so than the first paper,So the question really, if we can solve like, how do we have a proof? How do we have a real life data set? I think this is almost impossible,And if you think this is a quantum advantage, then maybe you will be disappointed,But if we relax some of these conditions, we can find very different arguments to say, I think we can put the interesting,It should be interesting

Also without this whole speed,All right,How do you find a pathological problem? I just saw this popping up,So I don't even know this is an English word, but in German studies, we always use this to talk about very carefully crafted artificial problems that we designed to be solveable by something,So something that you would never encounter in real life,In real life, there's always this nice real life world where, yeah, you know, there's obviously different levels of real life,There's something handcrafted as a data set

And there's MNIST,And then there is actually stuff that you really have in real life where you have to clean the data first and very noisy and then life is very different,Let me come to my last point,So we've got a little bit of time to discuss, but it's not very long,So the last one is the sentence you always have in paper,So we show with simulations that the quantum model learns well, then they use MNIST data, then often, because the algorithm is, as you know, if you can only simulate small,And MNIST is really big

And then you implement it on the IBM quantum experience with two qubits,And so those experiments and simulations are very hard to make,So I'm not taking anything away from this,And it's important that we get better and better at them,But I start to really wonder what they teach us,So if we can, so let's talk about it like this,So these benchmarks, they're basically classical machine learning technologies

So first of all, they're very small scale,And why? Because at the moment, we only have a few types of quantum computers or simulations available,So for example, our local simulations or simulations on big parallel clusters, the big parallel clusters, we can have a lot of qubits, but you have a lot of latency,So you can't stand a lot of jobs, which takes like really ages,So you have different like restrictions, but everything will be, let's say below 40 qubits, unless you have special tricks,Like the circuit is very simple or something,And then in hardware, I mean, our experiments, okay, I have to check the latest numbers, but like maybe between five and 10 qubits, and then you already have to spend 90% of your paper just mitigating noise

So we have very small and noisy studies,So that's the point,And if I now show that, oh, yeah,And then the second thing is I have to downsize the data because if I do something like MNIST, I usually turn my original data into something that's already a feature extraction, right? For example, quadrants here,And then we have like only two by two images, right? And those two by two images almost solve the problem already just by doing a linear regression,You're not even doing anything else or simply by counting pixels, you solve this problem to a really high degree already,And then you have to take a lot of time to really analyze that

So what I would like to see is either you take, you know, those real life datasets, and then you are very, very clear about how your pre and post processing actually influences your performance,Or what I would really like to see is that we use artificial data,And the point is we're physicists,So in artificial data, you can control parameters, you can have Gaussian blobs, and you can say, this is the variance of the blob, this is how separated there are,So you have a wonderful control field where you can actually set what you want,And you can analyze what's the effect on my algorithm,And then the question should be, how does this in my dataset change this in my algorithm? So basically, I kind of like try to summarize, and this is like something I also speak a lot with Amur about

So if you're writing a quantum machine learning paper, your next paper, I'm almost done here,And you actually try to show in your simulations, does feature map XX for variations? If I have a low test error on a downscaled MS dataset, why don't you replace it by question of, I kick out everything that's complex here about this,How does the spread of quantum states in Hilbert space that represents data change with the variance of an artificial dataset of Gaussian blobs? I think this is a much more interesting question for me,But I don't know if you will make it into nature with that, that's the problem,So publishers and supervisors might not, supervisors should, but publishers might not find this question very interesting,But I think this is very impressive,But I think to advance in quantum machine learning, we need to answer the second question

But few more of those that relates to the former section, how can I prove that my answer is classically intractable, which literally so much resources is spent on this question,But why don't we ask, for example, what is an ansatz design that allows gradient descent to scale as efficiently as neural networks? So the second one is an important question,The first one is maybe interesting to convince VCs that quantum computing is good to invest in,But, and this question we have to face at some stage, right? But at the moment, this is a lot more important for variation of training,And then I'll finally boil it down to how can I show that QML is more powerful? Super important question, maybe our central question,But I would argue that our important mandate as researchers, maybe not as companies, but as researchers is how can understand what quantum machine learning is doing,And I hope to see a lot more papers that have the, that's the answer

The second question instead of approved,Hold on,Okay, that's it,I hope I haven't ran it,Amira, can you please tell me if I was the negative? No, that was, that was awesome,Thank you,Thanks so much

I'll let Arusa like ask, there are millions of questions and very, a lot of compliments coming through about people enjoying your talk, Maria,So thank you so much,A lot of questions,It's not what Maria wanted,So should we take it from the top? Yeah, or you can pick out some that you think I definitely have a few in the list that I think are more relevant for this talk in particular,So a lot of people are asking also general questions,Yeah

But maybe the ones like I think one from Hannah is good,There are lots that are good,Okay, I found one that is regarding embedding,So it's from a him,He's asking, why can't we use amplitude embedding to use fewer qubits and thus avoid this quadratic increase in parameters? Okay, cool,So there's two answers to this question,I think the first one is that the, unless you train your embedding, the parameters are basically in most people's designs at least, they are in the part after the embedding

So this wouldn't, oh, I see what you mean,So then you train the circuit and you can have smaller circuits, right? Then there's a second question,So I think the first paper that I was involved in for, you know, this variation of circuits for machine learning, we used amplitude embedding,And afterwards, everyone always tried to do amplitude embedding,And the point was we never wanted to use it because it's powerful,We only wanted to use it because it controls your conditions, as I said at the beginning, because what's happening, you create a state because amplitude, so basically a vector that is exactly equal to your input vector, right? That's amplitude embedding,What's beautiful is that the state vector is obviously like kind of in quantum Hilbert space

So now you can act on it with qubits in kind of this like really insanely efficient way,But you basically now have a linear representation of your data and then you do linear transformations of that linear representation,It is a little bit more complicated than that, but not much more,So basically what you do, you just have a linear classifier,So you invest like millions of dollars into quantum computers just to do a linear classifier,And since you already loaded the data from somewhere, you already touched every feature,So your algorithm is already linear in the number of data unless you have a pure and magic fix

So in some sense, you know by definition that amplitude embedding can't gain you anything except from if you use very clever linear algebra support, there are a lot of arguments that can show you that somehow there's a scaling difference,But I think for variation circuits, this is not a good thing to use ever,So if we use some other techniques, some gradient-free optimization technique or we don't use the phase shift rule, can we still avoid this? I mean, I understand that amplitude embedding is, I also referred the person who asked this to your paper how quantum models are quantum methods and how amplitude embedding is not really doing much for you,But if you even ignore that and just take on the quadratic increase in parameters and their training,Yeah, no, good point,So at the moment, we don't have any other, so one has to be again always careful, right? When you train on hardware, when you don't have anything else than parameter shift rules, there are some relaxation if you want gradients out, there's some relaxations that you can sometimes cheat a bit by making use of lower shots or you only update one parameter at a time,So there's a lot of tricks, but there's actually no alternative for hardware

For software it's nice because you can just use TensorFlow, write your simulation in TensorFlow or PyTorch, which we are, I mean, everyone's doing,And then you can use automatic differentiation,So this crazy scaling from your networks, you can get it in simulations just because you can do the same tricks as in the networks, but you can't get it at the moment on quantum hardware,And then using parameter-free ideas, totally possible,I think we haven't explored this so much,The only thing is we know that in these high dimensional, really hard optimization problems that machine learning is solving, apparently there's no alternative,So when I ever see like a really good machine learning practitioner always asking, is there anything other than gradient descent? And they always said, you need gradient information because your space is also large

Maybe what could help us is to show that quantum circuits don't need lots of parameters that will be still worthwhile,And maybe QIA doesn't need so many, it's possible, right? But yeah, we haven't found this,Thank you,Okay, Tamara, Flora is yours,Cool,I have a question I thought was quite nice,This is by Renato Mello

So he asks, how do these results square with results from quantum signal processing? I think what he was referring to was the Fourier analysis stuff,How does this square with the quantum signal processing that shows that it is possible to use quantum circuits to express any real function? Yes, so you can, but you will just have to repeat your embedding,I think in our paper, there's actually proof that quantum circuits can be universal function approximators,If you either repeat your embedding, or what you can also do is you use a very, very, instead of encoding in a poly-exportation gates, you encode into a gate whose generator, so the Hamiltonian, has a really, really large spectrum,Then in theory, you can also like, you know, support all frequencies in the Fourier series,And if you then have a very expressive answer, so basically, if you hit enough resources into your quantum computer, you can get universal function approximated,But actually, I totally didn't ever look for quantum signal processing

And I love it the moment single processing, I find it so cool,So I will definitely look this up, because they probably have already found out everything that I want to know,And just to that point, somebody also asked, Karim asked, is this idea of repeating this repetition that you speak of, can we assume the same thing in the continuous variable setting, when we have like squeezing kernels or nonlinear gates, like this current cubic phase stuff? Yes, it always applies if you're embedding user's gates, where the features are encoded as the time evolution of Hamiltonian,So wherever you have, each of the i's times feature times Hamiltonian, and this is your encoding, which to some extent it always is, because even in amplitude embedding, how amplitude encoding works, if you simulate it, or if you put it in a computer, you kind of like do a long classical processing step to turn your original inputs into into phases and angles for poly rotations,And you have a very long circuit where you have the right poly angles,So you again end up with poly rotations,So all I'm saying is like, this is quite general approach

And then it's always true,And the spectrum of your Fourier series is then determined by basically the eigenvalue spectrum of your Hamiltonian,Perfect,Thank you,Arisa, go ahead,Yeah, maybe a related question to on the same line,In Yvonne Peters asks, in the paper where you describe how models arising from quantum circuits are just Fourier series, what's an intuitive explanation for how classically intractable functions arise from this model class? It feels like a classical computer could learn coefficients in a Fourier series efficiently, but we know that there is at least one circuit in this class that's classically intractable

I think I know what you mean,So I think there are different questions here,There's the question of how, there's actually a lot of questions in here, but somehow if you can, so I'm just thinking if you can represent a function, or if you can efficiently compute these two different questions,So this intractability question is really something different,So for example, you might have a function that a classical computer cannot efficiently evaluate for all inputs on average, or whatever your statement is there, but a quantum computer can't,So this is really not part of the statement,The statement is more like, if you had all the resources in the world, what could you in theory represent? That's a good point

It's a nice point of how this, you know, it's very hard sometimes to take those things apart,And also, the second part, it feels like a classical computer could learn for your coefficients efficiently,I don't think that's very true,Don't we get a speed up or transform when we use quantum computers? Yeah, so I obviously checked if there is, so there is actually, there's a lot of theory on how, at least in machine learning, so there's a couple of proposals of where you say like, why do we need to use a neural network, which you can also understand as some basis functions, and then you linearly combine them or something,Why don't we just use a Fourier series and learn the coefficients in some tensor representation, for example, which in my view now, this is what a lot of quantum variation circuits do,And I think that the devil's in the details,So obviously, you can't learn all the Fourier coefficients, but which one are important? How do you couple them? So this quantum classical separation there is not entirely clear to me how it relates to the Fourier series

And it's also not easy to compute what circuit leads to what Fourier coefficients,So we know how the equations work back, but they're very hard to work with,So someone has to go and make this easier, hopefully,Yeah, thanks,Sorry, I lost Maria's connection,Is it me? That's unstable,I think it's me, actually

For me, it's okay,But I'm sorry,Okay, Amiro, you want to take the next one? I think my connection is dying,So please, can you go ahead? I think there's something wrong with my internet,I think my parents are watching Netflix downstairs,Yeah, sure,So maybe we go towards the very, you know, the popular conception and that you tackle that quantum machine learning is not all about speed ups

And there's so much more that we need to understand,So there are a lot of questions regarding that and quantum advantage,So maybe we choose,Yeah, so there's one from Miguel Murca,I hope I'm saying it right,Is it possible to have provable quantum advantage without getting into the P equals NP debate? Yes,So again, like if you all these like points that are happening, when you all choose what you actually want to use in your statements, then there are papers that have provable advantages

And I think the nicest one to look at is, it's also one I showed,So it's a team from IBM,And it's called rigorous advanced quantum machine learning also,But it's a total pathological problem,So basically just chuck show into quantum machine learning,And there's actually another one also super nice by Ryan Zweger from the ENS Isatz group that do the same thing for generated models,But the idea is always that you put short or discrete log curves into a machine learning setting

And we have that linear story,Yes, we've got these proofs,I mean, are they important for machine learning, except from a conceptual way? Yeah, I think there's also like a distinction that we need to make when we talk about advantages and connect back to the learning theory, there's possible, like the two possible distinctions or classes we could look at as either advantages with respect to sample complexity or advantage with respect to competition complexity that you mentioned as well in your talk,And I think there's a bit of confusion there because I think so far, we know that there is not much advantage in sample complexity, but we still have hope in the other area,And there, then we try to cook up some really strict kind of controllable models that we can study that you were mentioning,Yeah, and also in the sample complexity space, so that's through all the, so what that means is basically can, basically this is the idea if you take data is not sampled from a classical probability distribution, but it's somehow sampled from a quantum computer,So it's basically like a quantum state, then you do something with a quantum state, and then you measure and these measurements aren't just samples

And then you can ask, do we need more or less samples in each case? You know, like, in the second,Don't we know now that even in that case, even if our data is produced from some classically intractable probability distribution, if we have enough data, classical machine learning models would be able to learn it,So there's, Although there's again the fine print, right? So for example, the recent paper by Robert Wang was showing that only on average, so it's again like the devil's in the detail, what was it again? On average or in the worst case have very different answers to this question,So there can be an exponential advantage in the one case or the other,The other one is like you can have polynomial advantages,I don't know if it helps anyone, but I sometimes think, okay, we don't know if all tolerant quantum computers are too expensive to like look for polynomial advantages, but I find it crazy if you've got a billion dimensional data set and you have something that scales quadratic with it,I mean, even if computers are fast, this is crazy, right? Or if it scales linear

So to me, this is all so quite interesting,And it's so difficult to put the setting right,So to understand in what setting the statement is actually for,So stop thinking, stop reading a paper and thinking only because they answered this positive or negative,It has anything to do with this question,Can quantum computers speed up machine learning because this question is still defined,Like just forget about this question

Just ask the precise question and then you'll see the answer,Yeah, but I think it's definitely very important thing to look at the computational complexity arguments then,Do we actually get any, anything better there? Okay, so Amira, should I still go on? Yeah, maybe we take one more and I share also quickly the next, the teaser for the next talk,So do you see the slide? So our next speaker, I hope you guys can hear me, my connection is terrible,The next speaker is David Sutter, who will be talking about exactly this idea of how powerful quantum neural networks are and the expressibility of these models,So we will send you a link to register for the next event, which will happen next year,So I'm going to send you a link to the event, which will happen on the 24th of June

You'll get that in your email and we'll also advertise this pretty soon,So please, I'll just go ahead and join you for the next talk,David, David looks like this actor, this actor in this photo,I'm just sure you've got the right,Yeah, I'm really sure,I stay at his face every day,Not by choice, but yeah

I mean, he's in my hands,That's definitely him,That's his photo that he uses for all his talks,Yes,Okay,Can we take one more question? Yeah,Yes

So let's stay on the topic of advantages,Let's see,There's so many questions,Yeah,Oh, and we should probably also say that we can copy the questions and send them over to Maria as well,And yeah, so that we can get a chance to get back to you,Okay

Maybe this question is nice,So Jonas Kagomo asked, how can we compare the speed of quantum algorithms with quantum input and output to classical algorithms with classical input and output? Yeah,Actually, I haven't even thought about that,So, you know, we think a lot about, not a lot, but there's obviously always this dubious idea of quantum data,So having quantum inputs, I mean, what that means in some sense is just that you have already your data in terms of a quantum state,So for example, you have a photon and it's in some coherent state and this is the here data set,And then you send it into a photonic quantum computer, for example, and then you act on it

But I've never thought about the output being quantum as well,So if the output is a classification, I don't know how that quantum output would even look like,That's a very interesting question,But sorry about these quantum learning from quantum data,So that was a bit the sample complexity debate,So that we are pretty sure that they are not very easy exponential speed ups,And this is quite surprising, actually, if we compare basically learning from quantum data or learning from classical data using quantum computer or classical computer

So yeah, so this we know,But other than that, I think quantum data might be really an important thing to use in the end quantum data stuff that we have access to with these machines,So maybe in some sense, quantum machine learning plus quantum sensing or quantum chemistry simulation, and then kind of like doing quantum machinery on top, maybe this is actually the best candidate of where we can do things,Well, like pointing find them here, like quantum computers will be quite good at simulating quantum systems,Yeah, yeah,Maybe the quantum output could be like some kind of post, post measurement kind of where one circuit is feeding into some other circuit or you know, reading of all the qubits,And so I don't know

At some stage you will have to measure,At some stage you will have to measure,Yeah, but this is generally a tricky question, like comparing classical and quantum models as a tricky area and it's not easy to exactly find analogues of your classical analogues of your model, right? Maybe to have a label of your data set as both yes and no, both cat and dog and then cancel your problem by just maybe not caring,Yeah, I think Hannah had a similar question,Should I go ahead and ask Hannah's question? Okay, maybe one last one,I think we're losing our participants here and I'm sure a lot of people want to have dinner and things,So last one, but yes, also thank you everyone for attending as well

Yeah,Okay, so Hannah asked a question along the same lines,What are good, fair ways to compare classical versus quantum machine learning architectures for showing potential quantum speedup? What does it make sense? Does it make sense to pick? Does it make sense to pick two with the same number of tunable parameters or weights or are there better, more systematic ways? So I personally think that this question doesn't have a good answer in the neural network sense simply because these models are not the same,How can you compare two mathematical things that are completely different? But in the kernel sense, this could be very well defined because you can compare the kernel matrices, you can compare, sorry, this is like a bit strange, but these reproducing kernel Hilbert spaces and what functions live in there and how big they are,So you have a very precise description of what functions they have in the kernel view,So this is why I find it, maybe this is why it's so hard to compare in the other,Yeah, yeah

Okay, Mira over to you,Thank you,No, cool,I was going to say, please wrap up and then join us again for the next quantum machine learning meetup in June,Cool,And then thank you, Maria,Mira, Arisa and Anta, who's not here, he's actually in a meeting where I'm also supposed to be

So thanks so much because I'm just here and ranting for an hour and you are organizing and you're supervising and you're mentoring and that's just so cool that you're doing all these things,So big, big thanks to you,And I love you lots,Love you too,Thanks so much,Thank you so much,Wonderful