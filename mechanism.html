<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="icon" href="./qiskit.svg" />
	<meta name="viewport" content="width=device-width" />

	<link rel="stylesheet" href="https://api.nukes.in/css/global.css">
	<link rel="stylesheet" href="https://api.nukes.in/css/keyframes.css">
	<link rel="stylesheet" href="/assets/style.css">

	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" crossorigin="anonymous">
	<meta http-equiv="content-security-policy" content="">
		<link href="./_app/immutable/assets/_layout.635835f1.css" rel="stylesheet">
		<link href="./_app/immutable/assets/_page.14a6c2ac.css" rel="stylesheet">
		<link rel="preload" as="script" crossorigin="anonymous" href="./_app/immutable/entry/start.7971b919.mjs">
		<link rel="preload" as="script" crossorigin="anonymous" href="./_app/immutable/chunks/index.486f8b79.mjs">
		<link rel="preload" as="script" crossorigin="anonymous" href="./_app/immutable/chunks/singletons.091bc144.mjs">
		<link rel="preload" as="script" crossorigin="anonymous" href="./_app/immutable/entry/app.9e2b6788.mjs">
		<link rel="preload" as="script" crossorigin="anonymous" href="./_app/immutable/chunks/preload-helper.41c905a7.mjs">
		<link rel="preload" as="script" crossorigin="anonymous" href="./_app/immutable/entry/_layout.svelte.8e2c013e.mjs">
		<link rel="preload" as="script" crossorigin="anonymous" href="./_app/immutable/chunks/nav.cdc405cf.mjs">
		<link rel="preload" as="script" crossorigin="anonymous" href="./_app/immutable/entry/_layout.js.fb1865a6.mjs">
		<link rel="preload" as="script" crossorigin="anonymous" href="./_app/immutable/chunks/_layout.da46b06b.mjs">
		<link rel="preload" as="script" crossorigin="anonymous" href="./_app/immutable/entry/mechanism-page.svelte.1447caf9.mjs"><!-- HEAD_svelte-1us85gb_START --><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"></script><!-- HEAD_svelte-1us85gb_END -->
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents">




<main class="svelte-1pvnnym"><nav class="fade-right svelte-1pvnnym" data-sveltekit-preload-data data-sveltekit-reload><a href="/" id="logo" class="∆í ‚àÜ-ct mx-a" style="padding: 20px 0 10px 0;display:block"><img src="qiskit.svg" alt="logo" height="32px" width="32px" style="margin-right: 5px;">
        <div class="fw4" style="font-size:24px;line-height:32px;align-self:center">new qkerns()
        </div></a>
      <hr class="w-50 o-25">
      <h4 class="p510 m0 svelte-1pvnnym">Pages</h4>
      <ul class="p520 svelte-1pvnnym"><li class="svelte-1pvnnym"><a href="/visualisations">Kern Visualisations</a></li>
        <li class="svelte-1pvnnym"><a href="/mechanism">Mechanism</a></li></ul>
      <h4 class="p510 m0 svelte-1pvnnym">üêç Python Files</h4>
        <ul class="p520 svelte-1pvnnym"><li class="svelte-1pvnnym"><a href="/python#qgss-kerns.html">QGSS QSVM (2021)</a>
            </li><li class="svelte-1pvnnym"><a href="/python#docs-hybrid.html">Std Hybrid Classifier</a>
            </li><li class="svelte-1pvnnym"><a href="/python#docs-kernlearn.html">Q Kernel Matrix Calc</a>
            </li><li class="svelte-1pvnnym"><a href="/python#docs-kerntrain.html">Q Kernel Train (QKA)</a>
            </li><li class="svelte-1pvnnym"><a href="/python#jupy-visualisations.html">Py Visualisations</a>
            </li><li class="svelte-1pvnnym"><a href="/python#kern-backprop.html">Kernel Backprop Demo</a>
            </li><li class="svelte-1pvnnym"><a href="/python#mnist-hybrid.html">MNIST Full Classifier</a>
            </li>
        </ul><h4 class="p510 m0 svelte-1pvnnym">üìù Markdown Files</h4>
        <ul class="p520 svelte-1pvnnym"><li class="svelte-1pvnnym"><a href="/markdown#summaries.md">Paper Summaries</a>
            </li><li class="svelte-1pvnnym"><a href="/markdown#updates.md">Weekly Updates</a>
            </li>
        </ul><h4 class="p510 m0 svelte-1pvnnym">üì∏ Docs &amp; Imgs</h4>
        <ul class="p520 svelte-1pvnnym"><li class="svelte-1pvnnym"><a href="/document#anglenc.pdf">Angle Encoding as Kern</a>
            </li>
        </ul><h4 class="p510 m0 svelte-1pvnnym">üñ•Ô∏è Presentation</h4>
        <ul class="p520 svelte-1pvnnym"><li class="svelte-1pvnnym"><a href="/presentation#w3.html">Week 3 Presentation</a>
            </li><li class="svelte-1pvnnym"><a href="/presentation#midsem.html">Midsem Presentation</a>
            </li><li class="svelte-1pvnnym"><a href="/presentation#mnist.html">Full MNIST Classification</a>
            </li>
        </ul></nav>
  <article class="mx-a p520 fade svelte-1pvnnym"><h1 class="w-100 mx-a">Kernel Functions Mechanism</h1>
<section class="svelte-2crdhv"><h2>Regression</h2>
  <p>Our objective first is to find the best linear predictor for the response
    variable $Y$ given the covariates $X$. We will assume that the response
    variable $Y$ is a linear combination of the covariates $X$ and derive a
    weight matrix $w$ such that <br>
    $$ Y = w^T X $$ We do this simply by defining the loss function as the sum of
    squared errors and minimizing it with respect to $w$. We can write this as
    <br>
    $$ J(w) = \min_w \sum_{i=1}^n (y_i - w^T x_i)^2 $$ Solving this for $w$
    gives us <br>
    $$ w = (X^T X)^{-1} X^T Y $$
  </p></section>
<section class="svelte-2crdhv"><h2>Non Linearity</h2>
  <p>We can extend this to non linear mappings for $X$ by introducing a function
    $\phi$ such that $X \rightarrow \phi(X)$. We can then write the desired
    predictor as <br>
    $$ Y = w^T \phi(X) $$ We can then also write the weight matrix as <br>
    $$ w^* = (\phi^T \phi)^{-1} \phi^T Y $$ Let us introduce a regularization
    term $\lambda$ such that the weight matrix is <br>
    $$ w = (\phi^T \phi + \lambda I)^{-1} \phi^T Y $$ The logic behind adding
    a regularization term is that we want to penalise the slope of the line. This
    is because we want to avoid overfitting. Adding bias results in a lower variance
    which makes the outputs less sensitive to the inputs. The parameter $\lambda$
    is what controls the amount of bias we want to add
    <br>
    We additionally notice that in order to calculate the weight matrix, we need
    also calculate $\phi^T \phi$ which is an $n \times n$ matrix. This is computationally
    expensive and we can instead use a &#39;kernel trick&#39; to avoid this. Before we do
    that, in the next section we will first set up the necessary background for the
    kernel trick.
  </p></section>
<details open class="svelte-2crdhv"><summary id="proof"><h2 class="d-ib m5">Restructuring the Weight Matrix</h2>
    <div class="mx-a"><b>Result:</b> <br>
      $ w^* = (\phi^T \phi + \lambda I)^{-1} \phi^T Y $ ‚Üí <br> $ w^*
      = \phi^T (\phi \phi^T + \lambda&#39; I)^{-1} Y $
    </div></summary>
  <p>Let us now define the new loss function along with a regularization term <br>
    $$ J(w) = \min_w \sum_{i=1}^n (y_i - w^T \phi(x_i))^2 + \frac{\lambda}2
    ||w||^2 $$ We can now solve for $w$ and get <br>
    $$ w^* = \frac1{\lambda} \sum_{i=1}^n (y_i - w^T \phi(x_i))
    \phi(x_i) $$ For sake of simplicity let us define a variable $\alpha$ such
    that <br>
    $$ \alpha = \frac1{\lambda} \sum_{i=1}^n (y_i - w^T
    \phi(x_i)) $$ We can now write the weight matrix as <br>
    $$ w^* = \sum_{i=1}^n \alpha_i \phi(x_i) = \phi^T \alpha $$ Let us now
    substitute this into the loss function and get <br>
    $$ J(\alpha) = (y - \phi \alpha)^T (y - \phi \alpha) + \frac{\lambda}2
    w^T w $$ expanding and simplifying this will give us <br>
    $$ J(\alpha) = y^T y - y^T \phi \phi^T \alpha - \alpha^T \phi^T y + \alpha^T
    \phi^T \phi \alpha + \frac{\lambda}2 w^T w $$ We can see that $\phi
    \phi^T$ is a repeated term. Let us define this new matrix as $K$ such that $$
    K = \phi \phi^T = \begin{bmatrix}
    \phi(x_1)^T \phi(x_1) &amp; \phi(x_1)^T \phi(x_2) &amp; \cdots &amp; \phi(x_1)^T \phi(x_n)
    \\ \phi(x_2)^T \phi(x_1) &amp; \phi(x_2)^T \phi(x_2) &amp; \cdots &amp; \phi(x_2)^T \phi(x_n)
    \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \phi(x_n)^T \phi(x_1) &amp; \phi(x_n)^T \phi(x_2)
    &amp; \cdots &amp; \phi(x_n)^T \phi(x_n) \end{bmatrix} $$ This matrix has two very
    important properties. First, it is symmetric and second, it is positive semi-definite.
    (This also means it is invertible which $\phi^T \phi$ MAY NOT). We can substitute
    all $\phi \phi^T$ with $K$ and also $K$ with $K^T$ and get <br>
    $$ J(\alpha) = y^T y - 2 y^T K \alpha + \alpha^T K^2 \alpha + \frac{\lambda}2
    \alpha^T K \alpha $$ Seeting the derivative of this with respect to $\alpha$
    to zero and solving for $\alpha$ gives us (along with $K = \phi \phi^T$)
    <br>
    $$ \alpha* = (K + \frac{\lambda}2 I)^{-1} y $$ or $$ \alpha* =
    (K + \lambda&#39; I)^{-1} y $$ We have achieved in this section effectively converting
    one equation to another as follows <br>
    $$ w^* = (\phi^T \phi + \lambda I)^{-1} \phi^T Y $$ into <br>
    $$ w^* = \phi^T (K + \lambda&#39; I)^{-1} Y $$ By the looks of it we may not
    have done anything, but as we will see in the next section, this step will reduce
    the computation time by a lot.
  </p></details>
<section class="svelte-2crdhv"><h2>Mercer&#39;s Theorem</h2>
  <p>A symmetric positive semi-definite function $K(x, y)$ can be expressed as an
    inner product of two vectors $\phi(x)$ and $\phi(y)$ such that <br>
    $$ K(x, y) = \langle \phi(x), \phi(y) \rangle $$ for some function $\phi$ iff
    $K(x,y)$ is positive semi-definite i.e <br>
    $$ \int K(x, y) g(x) g(y) dx dy \geq 0 \forall g $$ or equivalently <br>
    $$ \begin{bmatrix} K(x_1, x_1) &amp; K(x_1, x_2) &amp; \cdots \\ K(x_2, x_1) &amp; \ddots
    &amp; \\ \vdots &amp; &amp; \ddots \end{bmatrix} $$ is positive semi-definite for any
    collection ${x_1, x_2, \cdots}$
    <br></p></section>
<section class="svelte-2crdhv"><h2>The Kernel Trick</h2>
  <p>What Mercer&#39;s Theorem lets us do is rewrite every term in the Kernel matrix
    $K$ as only a function of its base features $$ K = \phi \phi^T = \begin{bmatrix}
    \phi(x_1)^T \phi(x_1) &amp; \phi(x_1)^T \phi(x_2) &amp; \cdots &amp; \phi(x_1)^T \phi(x_n)
    \\ \phi(x_2)^T \phi(x_1) &amp; \phi(x_2)^T \phi(x_2) &amp; \cdots &amp; \phi(x_2)^T \phi(x_n)
    \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \phi(x_n)^T \phi(x_1) &amp; \phi(x_n)^T \phi(x_2)
    &amp; \cdots &amp; \phi(x_n)^T \phi(x_n) \end{bmatrix} = \begin{bmatrix}
    k(x_1, x_1) &amp; k(x_1, x_2) &amp; \cdots &amp; k(x_1, x_n) \\ k(x_2, x_1) &amp; k(x_2, x_2)
    &amp; \cdots &amp; k(x_2, x_n) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ k(x_n, x_1) &amp;
    k(x_n, x_2) &amp; \cdots &amp; k(x_n, x_n) \end{bmatrix} $$
  </p>
  <ul class="d-ib"><li>First: $K$ is symmetric</li>
    <li>Second: $K$ is positive semi-definite (This also means it is invertible
      which $\phi^T \phi$ MAY NOT)
    </li></ul>
  <p>So as long as we know the Kernel (which we can either choose or learn), we
    can compute the Kernel matrix and use it to solve for $\alpha$ and then
    compute $w^*$ efficiently
  </p></section>
<section class="svelte-2crdhv"><h2>Working Example</h2>
  <p>Consider the following mapping $$ \phi: x \rightarrow \phi(x) = \begin{bmatrix}
    x_1^2 \\ \sqrt{2} x_1 x_2 \\ x_2^2 \end{bmatrix} $$ Let us for sake of
    demonstration work out its kernel <br>
    $$ \phi^T(x_m) \phi(x_n) = \begin{bmatrix} x_{m,1}^2 &amp; \sqrt{2} x_{m,1}
    x_{m,2} &amp; x_{m,2}^2 \end{bmatrix} \begin{bmatrix} x_{n,1}^2
    \\ \sqrt{2} x_{n,1} x_{n,2} \\ x_{n,2}^2 \end{bmatrix}
    = x_{m,1}^2 x_{n,1}^2 + 2 x_{m,1} x_{m,2} x_{n,1} x_{n,2}
    + x_{m,2}^2 x_{n,2}^2 $$ <br>
    Clearly $$ \phi^T(x_m) \phi(x_n) = (x_{m,1}
    x_{n,1} + x_{m,2} x_{n,2})^2 = (x_m^T x_n)^2 = k(x_m, x_n) $$
    This is an example of a Kernel called the <b>Polynomial Kernel</b> which is
    defined as $$ k(x, y) = (x^T y + r)^d $$ making in our case the parameters
    $d = 2$ and $r = 0$ <br></p></section>
<section class="svelte-2crdhv"><h2>Making Predictions</h2>
  <p>We can now make predictions using the Kernel trick. We can use the following
    equation to make predictions with $ y = w^T \phi(x) $ But as we have seen
    above we can convert the RHS from
    <br>$$ w^T \phi(x) = y(\phi \phi^T + \lambda&#39; I)^{-1} \phi^T \phi(x) $$
    to <br> $$ w^T \phi(x) = y(K + \lambda&#39; I)^{-1} k_x $$ where $k_x$ is
    <br>
    $$ k_x = \phi^T \phi(x) = \begin{bmatrix} \phi(x_1)^T \phi(x) \\ \phi(x_2)^T
    \phi(x) \\ \vdots \\ \phi(x_n)^T \phi(x) \end{bmatrix} $$ And our result
    is completely independent of the mapping $\phi$ and only depends on the Kernel
    $k$ and the data $X$ and $Y$ <br></p></section>
<section class="svelte-2crdhv"><h2>Applying the Kernel Trick to the SVM</h2>
  <p>As we know an SVM is a machine that can classify data by finding a
    hyperplane that separates the data into two classes. The SVM is a linear
    classifier, which means that it can only classify data that is linearly
    separable. But most data in the real world is not linear and so we need to
    use a non-linear classifier. The work around for that is that we first apply
    a non linear transformation to the data and then use a linear classifier to
    classify the transformed data. <br>
    Let us look at how we do that for a simple case
  </p>
  
  <pre data-language="python" class=" svelte-11sh29b"><code class="hljs"><!-- HTML_TAG_START --><span class="hljs-keyword">def</span> <span class="hljs-title function_">rand_pt_circle</span>(<span class="hljs-params">rad_min, rad_max</span>):
      angle = random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">2</span> * math.pi)
      radius = random.uniform(rad_min, rad_max)
      x = radius * math.cos(angle)
      y = radius * math.sin(angle)
      <span class="hljs-keyword">return</span> (x, y)

      <span class="hljs-comment"># Data points,</span>
      <span class="hljs-comment">#   X = listed 2D points of uniform random in circle of rad 0.6</span>
      <span class="hljs-comment">#   Y = listed 2D points of uniform random in annulus of rad 0.5 to 1</span>
      X = [rand_pt_circle(<span class="hljs-number">0</span>, <span class="hljs-number">0.6</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>)]
      Y = [rand_pt_circle(<span class="hljs-number">0.5</span>, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>)]

      fX = [(x[<span class="hljs-number">0</span>], x[<span class="hljs-number">1</span>], x[<span class="hljs-number">0</span>] ** <span class="hljs-number">2</span> + x[<span class="hljs-number">1</span>] ** <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X] <span class="hljs-comment"># THE TRANSFORMATION</span>

      <span class="hljs-comment"># fit</span>
      <span class="hljs-comment"># svm.SVC().fit(X,Y) rather than X,Y we use fX,Y</span>
      svm.SVC().fit(fX,Y)<!-- HTML_TAG_END --></code></pre>

  
  <img class="mx-a rx10" src="https://i.imgur.com/V6IWaD2.png" height="400px" width="400px">
  <p>While in an ideal world we should be able to stop here and call it a day, in
    reality we need to do a bit more work. The problem here is that choosing the
    the function fX here is a difficult task, and in interest of laziness we
    want this work cut out for us. The second problem is that in order to be
    able to make a non standard boundary we need to make a more complex non
    linear transform which in turn increases the computational requirements. The
    kernel trick will now be useful to us. The idea here is that the SVM itself
    does not need to know what each point is mapped to under this non linear
    transform, i.e $x_i \rightarrow f(x_i) \forall i$. The only thing it
    actually needs to know is how each point compares to each other data point
    i.e $f(x_i) vs f(x_j)$. This is still ultimately finding a glorified version
    of the distance between each point. Mathematically this is equivalent to
    doing $f(x_i)^T f(x_j)$ and this is what we define as the Kernel function
    <br> $$k(x_i, x_j) := f(x_i)^T f(x_j) $$
  </p>
  <h3>Examples</h3>
  <h4>Linear Kernel</h4>
  <p>Let us say the transform we intend to make is $f(x) = x$ i.e the identity
    transform. Then the kernel function is
    <br> $$ k(x_i, x_j) = x_i^T x_j $$ With the call
  </p>
  
  <pre data-language="python" class=" svelte-11sh29b"><code class="hljs"><!-- HTML_TAG_START -->svm.SVC(kernel=<span class="hljs-string">&#x27;linear&#x27;</span>).fit(X,Y)<!-- HTML_TAG_END --></code></pre>

  <p>The linear kernel gives us a flat decision boundary as expected, it can only
    make a straight line through the data without any transforms.
  </p>
  <h4>Polynomial Kernel</h4>
  <p>Let us say the non linear transform we intend to make is $f(x) = (x_1, x_2,
    x_1x_2, x_1^2 ,x_2^2)$ i.e the polynomial transform. Then the kernel
    function is
    <br> $$k(x_i, x_j) = (1 + x_i^T x_j)^2 $$ With the call
  </p>
  
  <pre data-language="python" class=" svelte-11sh29b"><code class="hljs"><!-- HTML_TAG_START -->svm.SVC(kernel=<span class="hljs-string">&#x27;poly&#x27;</span>, degree=<span class="hljs-number">2</span>).fit(X,Y)<!-- HTML_TAG_END --></code></pre>

  <p>The polynomial kernel gives us a curved decision boundary as expected, this
    is equivalent to first making an ideal transform before hand of the type
    $c_0 + c_1x_1 + c_2x_2 + c_3x_1x_2 + c_4x_1^2 + c_5x_2^2$ for some values
    $c_i$ and then using the linear kernel on the transformed data. The Kernel
    function here lets us find the relations between the values as if we had
    done the transform before hand without actually doing the transform.
  </p>
  <h4>RBF Kernel</h4>
  <p>What if we were to have a function for whom the transform were extremely
    difficult or impossible to calculate even in an approximate case, one such
    example is the Radial Basis Function (RBF) kernel. The RBF kernel is defined
    as
    <br>
    $$ k(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2) $$ With the call
  </p>
  
  <pre data-language="python" class=" svelte-11sh29b"><code class="hljs"><!-- HTML_TAG_START -->svm.SVC(kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, gamma=<span class="hljs-number">1</span>).fit(X,Y)<!-- HTML_TAG_END --></code></pre>

  <p>It turns out that the transform needed before hand for an RBF is infinite
    dimensional, i.e it looks like
    <br>
    $$ f(x) = (\text{infinite terms...}) $$ Notice how being basically
    impossible to calculate, we can still use the RBF kernel to find the
    relations between the points.
    <b>Note:</b> Gamma is a hyper parameter that controls the width of the RBF kernel.
    The smaller the gamma the wider the Kernel is therefore making it closer to a
    linear kernel. The larger the gamma the narrower the Kernel is therefore making
    it closer to a polynomial kernel of arbitrary degree. (See Proof in last section)
  </p></section>
<section class="svelte-2crdhv"><h2>Comparison with SINDy</h2>
  <div>This method is different from SINDy because SINDy usually aims to find the
    exact equations of the least number of degrees of freedom in any given
    system. We don&#39;t aim to do that. SINDy will not be able to convert a text
    description to an image. We want an arbitrary classifier that can be applied
    to any data set. SINDy would be much more suitable for a physical system
    where physics modelling is needed, ML is not needed for that and does not
    aim to solve those problems in the first place.
    <br>
    ML however can be used in places where even the Non Linear systems don&#39;t have
    a closed form solution and we want to predict the state at some far time t beyond
    the chaos boundary. The results, models or even the architectures of the ML models
    used here are however not well studied and is an active area of research.
    <ul><li>It is possible however that even for a given SINDy model, the ML model
        has lower computational requirements and thus is better used despite not
        being as accurate. A real world example was seen in Tesla Motors Inc
        where when calculating the current level of a battery it proved to be
        simpler to just use an ML model on the raw voltage than actually add
        physical resistors and model the complex interactions of all different
        sections, heat profiles and retentivities of the battery.
      </li>
      <li>As mentioned above, we have seen in Kadierdan et al CDC 2019, that SINDy
        can be very accurately used to model, and therefore balance an inverted
        vertical double pendulum. It however also turns out that SINDy is good
        at small specific classes of tasks and should be used for as such, it
        generalists very poorly as was seen in the modelling of Nuclear Fusion
        where fusion control is done much better with DeepMind&#39;s new model, we
        see that they&#39;re able to not only model and predict the gas plume
        behaviour but also control it. This is a much more complex task than the
        simple double pendulum and so it is not surprising that SINDy fails
        here. This is also a good example of how ML can be used to solve
        problems that are not solvable by SINDy.
      </li>
      <li>SINDy aims to find a simple model such that it is EXPLAINABLE. The
        latter being the more important part, in ML the model has no constraint
        on being explainable and is very happy being a black box in interest of
        which it has no constraints on what forms it can take, therefore
        structurally it can be much more complex than a SINDy model. This is a
        good thing because it allows us to model much more complex systems, but
        it is also a bad thing because it makes it harder to explain the model
        and thus harder to trust it. In interest of this ability to Generalise,
        in 2011 NASA switched to ML for aircraft engine premptive fault
        detection and in 2015 for drought prediction in the ECOSTRESS mission,
        both areas which were traditionally left to modelling.
      </li></ul></div></section>
<details class="svelte-2crdhv"><summary><h2 class="d-ib m5">Proof of RBF Kernel&#39;s Dimensionality</h2></summary>
  <div>$ k(x_i, x_j) $ <br>
    $ \quad = \exp(-\frac{1}{2} ||x_i - x_j||^2)$
    <br>
    $ \quad = \exp(-\frac{1}{2}
    \langle x_i - x_j\rangle^T \langle x_i - x_j\rangle) $ <br>
    $ \quad = \exp(-\frac{1}{2}
    (\langle x_i, x_i - x_j\rangle - \langle x_j, x_i - x_j\rangle)) $ <br>
    $ \quad = \exp(-\frac{1}{2}
    (\langle x_i, x_i\rangle - \langle x_i, x_j\rangle - \langle x_j, x_i\rangle
    + \langle x_j, x_j\rangle)) $ <br>
    $ \quad = \exp(-\frac{1}{2} (||x_i||^2 - 2\langle x_i, x_j\rangle + ||x_j||^2))
    $ <br>
    $ \quad = \exp[-\frac{1}{2}
    ||x_i||^2 - \frac{1}{2} ||x_j||^2] \exp(\langle x_i, x_j\rangle) $<br>
    $ \quad = C e^{\langle x_i, x_j\rangle}
    \quad \quad \text{since } C = \exp(-\frac{1}{2} ||x_i||^2 - \frac{1}{2}
    ||x_j||^2) $ <br> $ \quad = C \sum_{n=0}^{\infty} \frac{\langle x_i, x_j\rangle^n}{n!}
    \quad \quad \text{Taylor Series Expansion} $ <br>
    $ \quad = C \sum_{n=0}^{\infty}
    \frac{K_{poly(n)}(x_i, x_j) }{n!}$
  </div></details>
<h1>Kernel Generalisations</h1>
<section class="svelte-2crdhv"><h2>RBF $\rightarrow$ Matern</h2>
  <div>The RBF kernel is defined as
    <br> $$ k(x,x&#39;) = \exp\left(-\frac{d(x,x')}{2*ho^2}\right) $$
    where $d(x,x&#39;) \geq 0$ is the Euclidean distance between $x$ and $x&#39;$ and
    $\rho$ is a hyperparameter. The RBF kernel is infinitely differentiable and
    is positive definite.
    <br>
    The Matern kernel is a generalization of the RBF kernel. It is defined as
    <br> $$ k(x,x&#39;) = \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}d(x,x')}{\rho}\right)^\nu
    K_{\nu}\left(\frac{\sqrt{2\nu}d(x,x')}{\rho}\right) $$ where
    $K_{\nu}$ is the modified Bessel function of the second kind and
    $\Gamma$ is the gamma function. The parameter $\nu$ controls the smoothness
    of the function. For $\nu=1/2$, the Matern kernel reduces to the RBF kernel.
    For $\nu=1$, the Matern kernel reduces to the absolute exponential kernel.
    For $\nu\rightarrow\infty$, the Matern kernel reduces to the absolute
    exponential kernel. The Matern kernel is infinitely differentiable and is
    positive definite. The Matern kernel is also isotropic. The Matern kernel is
    defined for $d(x,x&#39;)\geq 0$.
  </div>
  <h3>Intuition</h3>
  <div>It&#39;s common to say that the Bessel functions are the solutions of the Bessel
    Differential Equation, but thats not much of an explanation. We arrive at
    the Bessel Differential Equation by transforming the wave equation into
    cylindrical co-ordinates.
    <br> <br>
    Intuitively The Bessel functions are what you get in two dimensions by taking
    superpositions of sine waves with circular symmetry. If you draw a circle 100
    meters in diameter, and put 1000 sources around the circumference of the circle,
    and have them transmit sine waves towards the center, all synchronized in phase,
    then the disturbance you get in the middle is described by a Bessel function
    ‚Ä¶ actually, $J_0$. If we do the same thing but have the source phase delayed
    linearly in a clockwise direction, so that when you come full circle they are
    back in phase again‚Ä¶that&#39;s the next Bessel function, $J_1$. Double the phase
    delay, and you get $J_2$, and so on.
    <br>
    So the RBF is effectively the same thing as a bessel function where on a membrane,
    rather than one &#39;dip&#39;, i.e as in dropping a ball in the center, In the Matern
    Kernel there may be an arbitrary amount of dips of varying hights say as when
    we beat a drum. The Gamma function is just a normalization factor which comes
    with some mathematical trickery.
    <br> <br>
    The main power of a Matern kernel is that it allows for non-stationary processes.
    The RBF kernel is stationary, meaning that the covariance between two points
    $x$ and $x&#39;$ is independent of the distance between them. The Matern kernel is
    non-stationary, meaning that the covariance between two points $x$ and $x&#39;$ is
    dependent on the distance between them making it much more flexible. Non-stationary
    kernels have proved to be very useful for modeling data that exhibit spatially
    varying behavior, such as weather patterns or population density. <br></div>
  <div><h4>Special Cases</h4>
    <ul><li><b>nu = 0.5:</b> Matern reduces to $e^{-d}$ i.e exponential distance
      </li>
      <li><b>nu = 1.5:</b> Matern reduces to $(1+ \sqrt(3) d) e^{-\sqrt(3) d}$
      </li>
      <li><b>nu = 2.5:</b> Matern reduces to $(1+ \sqrt(5) d + \frac53 d^2) e^{-\sqrt(5) d}$
      </li>
      <li><b>nu = $\infty$</b> Matern reduces to $e^{-\frac{d^2}{2}}$ i.e RBF
        kernel
      </li></ul></div>
</section>
    
    <p class="p520 svelte-1pvnnym"><br></p><p class="p520 svelte-1pvnnym"><br></p><p class="p520 svelte-1pvnnym"><br></p><p class="p520 svelte-1pvnnym"><br></p></article></main>
<div class="p-abs o-0 svelte-1pvnnym" id="overlay">
</div>


			
			<script>
				{
					__sveltekit_150kh81 = {
						env: {},
						assets: new URL(".", location.href).pathname.replace(/^\/$/, ''),
						element: document.currentScript.parentElement
					};

					const data = [null,null];

					Promise.all([
						import("./_app/immutable/entry/start.7971b919.mjs"),
						import("./_app/immutable/entry/app.9e2b6788.mjs")
					]).then(([kit, app]) => {
						kit.start(app, __sveltekit_150kh81.element, {
							node_ids: [0, 5],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
</body>


<script>
	window.TEXRender = () =>
		window.renderMathInElement( document.querySelector( "article.fade" ), {
			output: "html",
			delimiters: [
				{ left: "$$", right: "$$", display: true },
				{ left: "$", right: "$", display: false },
			],
			throwOnError: false,
		} );
</script>
<script type="module">
	// For mermaid on timeline page
	setTimeout( () => {
		document.querySelectorAll( '#timeline g.node' ).forEach( node =>
			node.setAttribute( 'transform', node.getAttribute( 'transform' ) + " scale(1.2)" )
		);
	}, 1000 )
</script>

</html>