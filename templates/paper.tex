\documentclass[hidelinks]{book}
%\usepackage{biblatex}
%\addbibresource{biblio.bib}
\usepackage{Resources/UoBLab}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{comment}

\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=blue,        % color of internal links
	citecolor=blue,        % color of links to bibliography
	filecolor=magenta,     % color of file links
	urlcolor=blue
}

\pubyear{2023}
\subjectarea{C-QuICC}

\begin{document}


\title{Quantum Machine Learning}
\author{Manav Seksaria, Supervisor: Anil Prabhakar}
\course{}
\school{Dept. of Electrical Engineering, IIT Madras}
\date{\today}
\keywords{NP-hard problems, quantum algorithms, machine learning}

\section{Introduction}
Machine learning focuses on the design and development of algorithms that can learn patterns from data and make predictions or decisions without being explicitly programmed. ML is used in a wide range of applications, from image and speech recognition to natural language processing and recommendation systems. Of all the multiple types of algorithms present in Machine Learning we will be focusing on the ones that are based on the concept of supervised learning. Supervised learning is a type of machine learning algorithm that is used to learn a function that maps an input to an output based on example input-output pairs.

The goal of supervised learning is to find a function that can accurately predict the output for new inputs. Supervised learning algorithms can be divided into generally two main categories: classification and regression. Classification algorithms are used to predict a discrete label for a given input, while regression algorithms are used to predict a continuous value for a given input. Even within the classification category of algorithms we will be focusing on the ones that are based on the concept of Support Vector Machines (SVMs). The main idea behind SVMs is to find the hyperplane that separates the data into classes in the high-dimensional feature space, while maximizing the margin between the two classes. This is achieved by finding the support vectors, which are the data points closest to the hyperplane and have the greatest impact on its position.

\subsection{Working of SVM}
\begin{enumerate}
	\item Data preparation: Prepare the training data by transforming it into a suitable format for the SVM algorithm to work with. This typically involves normalizing the features and splitting the data into training and validation sets. \newline
	\textit{Working Ex}: In a 2-class classification problem, the data is split into two classes and labeled as class 1 and class 2, with each data point representing a feature vector in a multi-dimensional feature space.

	\item Choosing the right kernel function: Select the appropriate kernel function to be used in the SVM algorithm based on the structure of the data. Different kernel functions are used for different types of data, as they determine how the data is transformed into a higher-dimensional space for classification. \newline
	\textit{Working Ex}: For non-linearly separable data, a radial basis function (RBF) kernel is often selected, as it maps the data into a higher-dimensional feature space where a linear hyperplane can effectively separate the classes.

	\item Training the SVM: Train the SVM model on the training data using the selected kernel function. The SVM algorithm finds the hyperplane that best separates the two classes in the feature space, based on the selected kernel function and the input data. \newline
	\textit{Working Ex}: The SVM algorithm iteratively finds the optimal hyperplane that maximizes the margin between the two classes, by adjusting the weights and bias of the hyperplane and using optimization techniques such as gradient/subgradient descent.

	\item Hyperplane selection: Find the hyperplane that best separates the two classes in the feature space, by maximizing the margin between the classes. The SVM algorithm selects the hyperplane that is maximally distant from the closest data points in each class, known as support vectors. \newline
	\textit{Working Ex}: The selected hyperplane acts as the decision boundary that separates the two classes, with data points on either side of the hyperplane being classified as belonging to class 1 or class 2.

	\item Class prediction: Use the trained SVM model to make predictions on new, unseen data. The trained SVM model can be used to classify new data points based on their position relative to the hyperplane found in the previous step. \newline
	\textit{Working Ex}: A new data point is fed into the trained SVM model, and the algorithm predicts whether it belongs to class 1 or class 2 based on its position relative to the hyperplane. The prediction is made by evaluating the dot product of the new data point and the weights of the hyperplane, with a positive value indicating class 1 and a negative value indicating class 2.

	\item Validation: Evaluate the performance of the SVM model on a validation set to determine how well it generalizes to new data. The performance of the SVM model is typically measured using metrics such as accuracy, precision, recall, and F1 score. \newline
	\textit{Working Ex}: The accuracy of the SVM model is measured on a validation set, and the results are used to fine-tune the model and improve its performance. The validation set is used to assess the generalization error of the model, which is a measure of how well the model performs on unseen data.
\end{enumerate}

\section{SVMs & Kernels}
\textbf{Kernels:} A kernel is a mathematical function that measures the similarity between two data points. Kernels are used to map the data into a higher dimensional space, where it may become linearly separable and easier to classify.

\textbf{Kernel Functions:} A kernel function is a specific type of function that can be used as a kernel. Common examples of kernel functions include the radial basis function (RBF) kernel, the polynomial kernel, and the sigmoid kernel.

\textbf{Kernel Machines:} A kernel machine is a type of machine learning algorithm that makes use of kernel functions to perform nonlinear transformations of the data. Some examples of kernel machines are:

\begin{itemize}
	 \item \textit{Support Vector Machines} (SVMs): As mentioned above, SVMs are a type of algorithm that uses a kernel function to transform the data into a higher dimensional space where it becomes linearly separable.

	 \item \textit{Gaussian Processes} (GPs): Gaussian processes are a type of probabilistic model that can be used for regression and classification tasks. They make use of a kernel function to define the covariance between the data points.

	 \item \textit{Kernel Principal Component Analysis} (KPCA): KPCA is a type of dimensionality reduction technique that uses a kernel function to transform the data into a lower dimensional space while preserving the most important features.
\end{itemize}

Each of these kernel machines has a unique set of advantages and disadvantages and can be used in different types of applications depending on the problem being solved and the characteristics of the data.

\subsection{Most Common Kernels}

\begin{itemize}
	\item \textbf{Linear Kernel:}
	\begin{align}
	$K(x_i, x_j) = x_i^T x_j$
	\end{align}
	The linear kernel is the simplest type of kernel function and is used when the data is already linearly separable in the original feature space. The intuition behind the linear kernel is that it computes the dot product between two data points, which measures the similarity between them.

	\item \textbf{Polynomial Kernel:}
	\begin{align}
	$K(x_i, x_j) = (x_i^T x_j + c)^d$
	\end{align}
	The polynomial kernel is used when the data is not linearly separable in the original feature space. The intuition behind the polynomial kernel is that it raises the dot product between two data points to a specified power, which allows for the creation of nonlinear boundaries between classes. The parameter $c$ is used to shift the decision boundary, and the parameter $d$ determines the degree of the polynomial.

	\item \textbf{Radial Basis Function (RBF) Kernel:}
	\begin{align}
	$K(x_i, x_j) = \exp(\frac{-||x_i - x_j||^2}{2 l^2})$
	\end{align}
	The radial basis function (RBF) kernel is a popular choice for nonlinear classification and regression problems. The intuition behind the RBF kernel is that it measures the similarity between two data points based on the Euclidean distance between them. The parameter $\gamma$ determines the shape of the decision boundary, with larger values resulting in a tighter boundary around each data point.

	Common variants of the RBF kernel include the Gaussian kernel, Cauchy kernel, and Laplacian kernel (or Laplacian of Gaussian).

	\item \textbf{Sigmoid Kernel:}
	\begin{align}
	$K(x_i, x_j) = \tanh(\alpha x_i^T x_j + c)$
	\end{align}
	The sigmoid kernel is similar to the polynomial kernel, but with a nonlinear transformation of the dot product between two data points. The intuition behind the sigmoid kernel is that it maps the dot product between two data points to a value between -1 and 1, which can be used to create nonlinear decision boundaries. The parameters $\alpha$ and $c$ are used to control the shape of the sigmoid function.

	\item \textbf{Periodic Kernel:}
	\begin{align}
	$K(x_i, x_j) = \exp\left(-\frac{2 \sin^2(\pi ||x_i - x_j|| / p)}{l^2}\right)$
	\end{align}
	The periodic kernel is a type of kernel function that is used to model the behavior of periodic functions. The periodic kernel is used to model the behavior of periodic functions, which are a type of special function that can be used to model the behavior of waves.

	Common variants of the periodic kernel include the exponential sin squared kernel and the Mat√©rn kernel.
\end{itemize}

\subsection{Relation to Convolution Filters}
Convolution filters are a fundamental tool in image processing for transforming and analyzing digital images. Convolution filters are used to apply a mathematical operation to an image, resulting in a new image that has been transformed in some way.

Convolution filters are generalized by the convolution operation, which is a mathematical operation that applies a filter to an image by multiplying the filter values with the image values and summing the results. The result of the convolution operation is a new image that has been transformed by the filter.

Convolution filters are related to kernel functions in that they both represent a set of values that are used to perform a mathematical operation on an input signal. In the case of image processing, the input signal is an image, and the operation is the convolution operation. The values in the convolution filter can be thought of as the coefficients of a kernel function, and the convolution operation can be thought of as applying the kernel function to the image.

In general, kernel functions and convolution filters are used to extract features from an input signal, such as an image, by applying a mathematical operation that emphasizes certain aspects of the signal. Convolution filters are particularly useful in image processing because they allow for the application of a wide range of image transformations, including edge detection, smoothing, and sharpening.


\end{document}